# Flink中Kafka连接器的使用

**1 需求及其解决方案**

--1.1 项目需求

- 从Kafka中读取sflow报文获得流量流

--1.2 解决方案

- 基于Flink提供的Kafka连接器消费Topic消息

**2 Kafka连接器使用说明**

- Flink 提供了专门的 Kafka 连接器，向 Kafka topic 中读取或者写入数据。Flink Kafka Consumer 集成了 Flink 的 Checkpoint 机制，可提供 exactly-once 的处理语义。为此，Flink 并不完全依赖于跟踪 Kafka 消费组的偏移量，而是在内部跟踪和检查偏移量

--2.1 Kafka Consumer

- Flink 的 Kafka consumer 称为 FlinkKafkaConsumer08（或适用于 Kafka 0.9.0.x 版本的 FlinkKafkaConsumer09，或适用于 Kafka >= 1.0.0 的版本的 FlinkKafkaConsumer）。它提供对一个或多个 Kafka topics 的访问

- 构造参数
   - Topic 名称或者名称列表
   - 用于反序列化 Kafka 数据的 DeserializationSchema 或者 KafkaDeserializationSchema
   - Kafka 消费者的属性。需要以下属性：
      - “bootstrap.servers”（以逗号分隔的 Kafka broker 列表）
      - “group.id” 消费组 ID
      - “zookeeper.connect”（以逗号分割的 Zookeeper servers 列表) (仅 Kafka 0.8 需要)

- 实例
~~~
Properties properties = new Properties();
properties.setProperty("bootstrap.servers", "localhost:9092");
// 仅 Kafka 0.8 需要
properties.setProperty("zookeeper.connect", "localhost:2181");
properties.setProperty("group.id", "test");
DataStream<String> stream = env
  .addSource(new FlinkKafkaConsumer08<>("topic", new SimpleStringSchema(), properties));
~~~

--2.2 DeserializationSchema

- Flink Kafka Consumer 需要将 Kafka 中的二进制数据转换为 Java 或者 Scala 对象。DeserializationSchema 允许用户指定这样的 schema，为每条 Kafka 消息调用 T deserialize(byte[] message) 方法，传递来自 Kafka 的值

- 为了访问 Kafka 消息的 key、value 和元数据，KafkaDeserializationSchema 具有以下反序列化方法 T deserialize(ConsumerRecord<byte[], byte[]> record)
   - TypeInformationSerializationSchema（和 TypeInformationKeyValueSerializationSchema) 基于 Flink 的 TypeInformation 创建 schema
   - JsonDeserializationSchema（和 JSONKeyValueDeserializationSchema）将序列化的 JSON 转化为 ObjectNode 对象，可以使用 objectNode.get("field").as(Int/String/...)() 来访问某个字段
   - AvroDeserializationSchema 使用静态提供的 schema 读取 Avro 格式的序列化数据

--2.3 配置 Kafka Consumer 开始消费的位置

- Flink Kafka Consumer 的所有版本都具有明确的起始位置配置方法
   - setStartFromGroupOffsets（默认方法）：从 Kafka brokers（或者从 Kafka 0.8 版本的 Zookeeper 中）中的 consumer 组（consumer 属性中的 group.id 设置）提交的偏移量中开始读取分区。 如果找不到分区的偏移量，那么将会使用配置中的 auto.offset.reset 设置
   - setStartFromEarliest() 或者 setStartFromLatest()：从最早或者最新的记录开始消费，在这些模式下，Kafka 中的 committed offset 将被忽略，不会用作起始位置
   - setStartFromTimestamp(long)：从指定的时间戳开始。对于每个分区，其时间戳大于或等于指定时间戳的记录将用作起始位置。如果一个分区的最新记录早于指定的时间戳，则只从最新记录读取该分区数据。在这种模式下，Kafka 中的已提交 offset 将被忽略，不会用作起始位置

- 实例
~~~
final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

FlinkKafkaConsumer08<String> myConsumer = new FlinkKafkaConsumer08<>(...);
myConsumer.setStartFromEarliest();     // 尽可能从最早的记录开始
myConsumer.setStartFromLatest();       // 从最新的记录开始
myConsumer.setStartFromTimestamp(...); // 从指定的时间开始（毫秒）
myConsumer.setStartFromGroupOffsets(); // 默认的方法

DataStream<String> stream = env.addSource(myConsumer);
...
~~~

- 也可以为每个分区指定 consumer 应该开始消费的具体 offset
   - 实例
   ~~~
    Map<KafkaTopicPartition, Long> specificStartOffsets = new HashMap<>();
    specificStartOffsets.put(new KafkaTopicPartition("myTopic", 0), 23L);
    specificStartOffsets.put(new KafkaTopicPartition("myTopic", 1), 31L);
    specificStartOffsets.put(new KafkaTopicPartition("myTopic", 2), 43L);

    myConsumer.setStartFromSpecificOffsets(specificStartOffsets);
   ~~~
   - 上面的例子中使用的配置是指定从 myTopic 主题的 0 、1 和 2 分区的指定偏移量开始消费。offset 值是 consumer 应该为每个分区读取的下一条消息。请注意：如果 consumer 需要读取在提供的 offset 映射中没有指定 offset 的分区，那么它将回退到该特定分区的默认组偏移行为（即 setStartFromGroupOffsets()）

--2.4 Kafka Consumer 和 时间戳抽取以及 watermark 发送

- 在许多场景中，记录的时间戳是(显式或隐式)嵌入到记录本身中。此外，用户可能希望定期或以不规则的方式 Watermark，例如基于 Kafka 流中包含当前事件时间的 watermark 的特殊记录。对于这些情况，Flink Kafka Consumer 允许指定 AssignerWithPeriodicWatermarks 或 AssignerWithPunctuatedWatermarks

- 在内部，每个 Kafka 分区执行一个 assigner 实例。当指定了这样的 assigner 时，对于从 Kafka 读取的每条消息，调用 extractTimestamp(T element, long previousElementTimestamp) 来为记录分配时间戳，并为 Watermark getCurrentWatermark()（定期形式）或 Watermark checkAndGetNextWatermark(T lastElement, long extractedTimestamp)（打点形式）以确定是否应该发出新的 watermark 以及使用哪个时间戳

- 实例
~~~
Properties properties = new Properties();
properties.setProperty("bootstrap.servers", "localhost:9092");
// 仅 Kafka 0.8 需要
properties.setProperty("zookeeper.connect", "localhost:2181");
properties.setProperty("group.id", "test");

FlinkKafkaConsumer08<String> myConsumer =
    new FlinkKafkaConsumer08<>("topic", new SimpleStringSchema(), properties);
myConsumer.assignTimestampsAndWatermarks(new CustomWatermarkEmitter());

DataStream<String> stream = env
  .addSource(myConsumer)
  .print();
~~~

**3 项目中的实际使用**

--3.1 ZooKeeper中Kafka的配置

~~~
kafka:
  url: xx0:6667,xx1:6667,xx2:6667
  flow-topic: vflow.sflow
~~~

--3.2 项目中Kafka连接器的依赖

~~~
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-connector-kafka-0.10_${scala.binary.version}</artifactId>
    <version>${flink.version}</version>
</dependency>
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-streaming-java_${scala.binary.version}</artifactId>
    <version>${flink.version}</version>
    <scope>provided</scope>
</dependency>
~~~

--3.3 项目中使用Kafka连接器消费的具体代码举例

- 流量统计主函数调用Kakfa连接器获得流量流

~~~
    FMConfig config = new FMConfig();
    TrafficConfig tc = config.loadModuel(TrafficConfig.PREFIX, p, TrafficConfig.class);

    KafkaReader<SFlowRecord> reader = new KafkaReader<>();
    DataStream<SFlowRecord> baseFlow = reader.readDataFromKafka(env, tc.getKafkaConfig(), "flink-group",
            new SFlowJson2FlowRecordMap(), "kafka-reader");
~~~

- KafkaReader的定义实现

~~~
public class KafkaReader<T> {

    int waterMarkPeriodMilSecond = 1*1000;
    public KafkaReader(int waterMarkPeriodMillSecond){
        this.waterMarkPeriodMilSecond = waterMarkPeriodMillSecond;
    }
    public KafkaReader(){}


    /***
     * Read data from kafka and add watermark every 10 secondes
     * @param env global environment
     * @param kafkaConfig kafka config data structure;
     * @param groupId kafka group id
     * @param mapper how to transfer a message from kafka to a T
     * @param name name of mapper
     * @return a new DataStream of T
     */
    public DataStream<T> readDataFromKafka(StreamExecutionEnvironment env,
                                           KafkaConfig kafkaConfig,
                                           String groupId,
                                           FlatMapFunction<String, T> mapper,
                                           String name) {

      return readDataFromKafka(env,kafkaConfig.getUrls(),groupId,kafkaConfig.getTopic(),mapper,name);
    }

    /***
     * Read data from kafka and add watermark every 10 secondes
     * @param env global environment
     * @param kafkaUrl kafka brokers url xx0:6667,xx1:6667,xx2:6667
     * @param groupId kafka group id
     * @param topic what topic should read from kafka
     * @param mapper how to transfer a message from kafka to a T
     * @param name name of mapper
     * @return a new DataStream of T
     */
    public DataStream<T> readDataFromKafka(StreamExecutionEnvironment env,
                                           String kafkaUrl,
                                           String groupId,
                                           String topic,
                                           FlatMapFunction<String, T> mapper,
                                           String name) {

        Properties props = new Properties();
        props.setProperty("bootstrap.servers", kafkaUrl);
        props.setProperty("group.id", groupId);
        FlinkKafkaConsumer010<String> consumer =
                new FlinkKafkaConsumer010<>(topic, new SimpleStringSchema(), props);

        DataStream<String> kafkaInput;
        if (name != null)
            kafkaInput = env.addSource(consumer).name(name);
        else
            kafkaInput = env.addSource(consumer);
        SingleOutputStreamOperator baseFlow =
                ((DataStreamSource<String>) kafkaInput)
                        .setParallelism(1)
                        .rebalance()
                        .flatMap(mapper)
                        .name(name)
                        .assignTimestampsAndWatermarks(new TimestampedRecordWatermarkEmitter(this.waterMarkPeriodMilSecond));
        return baseFlow;
    }

}
~~~

- TimestampedRecordWatermarkEmitter的定义实现

~~~
public class TimestampedRecordWatermarkEmitter<T extends TimestampedRecord> implements AssignerWithPunctuatedWatermarks<T> {
    private long waterMarkPeriodMillSecond;
    private long lastWaterMarkMillSecond = -1;
    public TimestampedRecordWatermarkEmitter(int waterMarkPeriodMillSecond){
        this.waterMarkPeriodMillSecond = waterMarkPeriodMillSecond;
    }

    @Nullable
    @Override
    public Watermark checkAndGetNextWatermark(TimestampedRecord lastElement, long extractedTimestamp) {
        if(extractedTimestamp - lastWaterMarkMillSecond >= waterMarkPeriodMillSecond){
            lastWaterMarkMillSecond = extractedTimestamp;
            return new Watermark(extractedTimestamp);
        }
        return null;
    }

    @Override
    public long extractTimestamp(T element, long previousElementTimestamp) {
        return element.getTimestamp()*1000;
    }
}
~~~

**4 参考**

- [Apache Kafka 连接器](https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/dev/connectors/kafka.html)