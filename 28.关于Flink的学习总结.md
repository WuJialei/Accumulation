# 关于Flink的学习总结

**1 Flink的概念**

--1.1 基本概念

- 分布式

- 支持流处理和批处理

- 开源计算平台

--1.2 特点

- 统一的批处理和流处理系统

- Flink流处理的容错机制

- Flink流处理的时间窗口

**2 Flink的架构**

--2.1 基本架构

2.1.1 Flink包含了两种类型的处理器

- JobManager(Master)
   - 负责接收 flink 的作业，调度task，协调检查点，协调失败时恢复等
   - 收集 job 的状态、管理 TaskManagers
   - Flink运行时至少存在一个master处理器，如果配置高可用模式则会存在多个master处理器，它们其中有一个是leader，其他的都是standby

- TaskManager(Worker)
   - Flink中资源管理的基本组件，是所有执行任务的基本容器，提供了内存管理、IO管理、通信管理等一系列功能
   - MemoryManager Flink并没有把所有内存的管理都委托给JVM，因为JVM普遍存在着存储对象密度低、大内存时GC对系统影响大等问题。所以Flink自己抽象了一套内存管理机制，将所有对象序列化后放在自己的MemorySegment上进行管理
   - IOManager flink通过IOManager管理磁盘IO的过程，提供了同步和异步两种写模式，又进一步区分了block、buffer和bulk三种读写方式
   - NetworkEnvironment 是TaskManager的网络 IO 组件，包含了追踪中间结果和数据交换的数据结构。它的构造器会统一将配置的内存先分配出来，抽象成 NetworkBufferPool 统一管理内存的申请和释放
   - Flink运行时至少会存在一个worker处理器

2.1.2 Flink中计算资源的介绍

- TaskManager：Flink中资源管理的基本组件，是所有执行任务的基本容器
   - 每个TaskManager都是一个独立的JVM进程

- TaskManager可以分为一个或者多个Task Slot：每个Task Slot代表了TaskManager的一个固定大小的资源子集。例如，一个拥有3个slot的 TaskManager，会将其管理的内存平均分成三分分给各个slot。将资源slot化意味着来自不同job的task不会为了内存而竞争，而是每个task都拥有一定数量的内存储备。
   - 同一JVM(TaskManager)中的任务(Task Slot)共享TCP连接和心跳消息
   - TaskManager的一个Slot代表一个可用线程，该线程具有固定的内存，注意 Slot 只对内存隔离，没有对CPU隔离
   - 每个slot能运行一个或多个task(一个task是一个线程，task是执行计算的最小结构)
   - 每个TaskManager的Task Slot数目代表了该TaskManager的并发能力，任务的并发能力由所有TaskManager的Task Slot数目决定。有两个 Task Manager，每个 TaskManager有三个slot，这样我们的算子最大并行度那么就可以达到6个
   - slot 的数量通常与每个TaskManager的可用CPU内核数成比例。一般情况下你的slot数是每个TaskManager的cpu的核数
   - parallelism与slot的区别，参考[Flink--对parallelism 和 slot的理解](https://www.jianshu.com/p/b58988bcfb48)：slot是指taskmanager的并发执行能力；parallelism是指 taskmanager 实际使用的并发能力(并行度)，一个特定operator的subtask的个数被称之为其parallelism(并行度)；taskmanager.numberOfTaskSlots:3；即每一个 taskmanager 中的分配3个TaskSlot, 3个 taskmanager 一共有 9 个 TaskSlot。parallelism.default:1；即运行程序默认的并行度为 1，9 个 TaskSlot 只用了 1 个，有 8 个空闲。设置合适的并行度才能提高效率

- Task Slot允许多个task共享：每个Task Slot能运行一个或多个task，为了资源更充分的利用，Flink提出了SlotSharingGroup，尽可能地让多个task共享一个slot
   - 条件：它们都来自同一个Job的不同task的subtask(同一job的一个task下的相同subtask应该并行，不能放在同一个slot中)
   - slot共享有以下两点好处：1.Flink集群所需的task slots数与job中最高的并行度一致。也就是说我们不需要再去计算一个程序总共会起多少个task了；2.更容易获得更充分的资源利用。如果没有slot共享，那么非密集型操作source/flatmap就会占用同密集型操作 keyAggregation/sink 一样多的资源。如果有slot共享，将基线的2个并行度增加到6个，能充分利用slot资源，同时保证每个TaskManager能平均分配到重的subtasks(该部分看参考链接的图)

- 算子operators的Chaining：Flink提出了Chaining，尽可能地将operators chain在一起作为一个task来处理
   - Flink会尽可能地将operator的subtask链接（chain）在一起形成task。每个task在一个线程中执行。
   - 好处：将operators链接成task是非常有效的优化，它能减少线程之间的切换，减少消息的序列化/反序列化，减少数据在缓冲区的交换，减少了延迟的同时提高整体的吞吐量
   - 不是任意两个operator就能chain一起的，其条件还是很苛刻的(具体条件可看链接)
   - 出于分布式执行的目的，Flink将operator的subtask链接在一起形成task，每个task在一个线程中执行。将operators链接成task是非常有效的优化：它能减少线程之间的切换和基于缓存区的数据交换，在减少时延的同时提升吞吐量。链接的行为可以在编程API中进行指定


--2.2 运行架构

2.2.1 Flink的三种图结构(该部分主要参考[追源索骥：透过源码看懂Flink核心框架的执行流程之2.理解flink的图结构](https://www.cnblogs.com/bethunebtj/p/9168274.html#2%E7%90%86%E8%A7%A3flink%E7%9A%84%E5%9B%BE%E7%BB%93%E6%9E%84))

- StreamGraph
   - StreamGraph是对用户逻辑的映射
   - 代表程序的拓扑结构，是从用户代码直接生成的图
   - client执行env.execute()时生成

- JobGraph
   - JobGraph在StreamGraph基础上进行了一些优化，比如把一部分操作串成chain以提高效率
   - client生成
   - ExecutionGraph已经可以用于调度任务。我们可以看到，flink根据该图生成了一一对应的Task，每个task对应一个ExecutionGraph的一个Execution。Task用InputGate、InputChannel和ResultPartition对应了上面图中的IntermediateResult和ExecutionEdge

- ExecutionGraph
   - ExecutionGraph是为了调度存在的，加入了并行处理的概念
   - JobManager处生成，入口代码是ExecutionGraphBuilder.buildGraph（...）

2.2.2 Flink任务执行架构图

- 参考官网[Job Managers、Task Managers、客户端（Clients）](https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/concepts/runtime.html)

2.2.3 Flink任务的调度和执行

- Flink 集群启动后，首先会启动一个 JobManger 和多个的 TaskManager。用户的代码会由JobClient 提交给 JobManager，JobManager 再把来自不同用户的任务发给 不同的TaskManager 去执行，每个TaskManager管理着多个task，task是执行计算的最小结构， TaskManager 将心跳和统计信息汇报给 JobManager。TaskManager 之间以流的形式进行数据的传输。上述除了task外的三者均为独立的 JVM 进程。
- 要注意的是，TaskManager和job并非一一对应的关系。flink调度的最小单元是task而非TaskManager，也就是说，来自不同job的不同task可能运行于同一个TaskManager的不同线程上


--2.3 备注：该部分主要参考以下内容

- [Cris 带你快速入门 Flink之二 Flink基本架构](https://juejin.im/post/5c4f16dbe51d454f342fb7e7#heading-4)
- [追源索骥：透过源码看懂Flink核心框架的执行流程之3. 任务的调度与执行](https://www.cnblogs.com/bethunebtj/p/9168274.html#3-%E4%BB%BB%E5%8A%A1%E7%9A%84%E8%B0%83%E5%BA%A6%E4%B8%8E%E6%89%A7%E8%A1%8C)
- [Flink 原理与实现：理解 Flink 中的计算资源](http://wuchong.me/blog/2016/05/09/flink-internals-understanding-execution-resources/)


**3 统一的批处理和流处理系统**

--3.1 流处理系统与批处理系统的不同

- 最大不同在于节点间的数据传输方式

- 流处理系统，节点间数据传输模型：当一条数据被处理完成后，序列化到缓存中，然后立刻通过网络传输到下一个节点，由下一个节点继续处理

- 批处理系统，其节点间数据传输模型是：当一条数据被处理完成后，序列化到缓存中，并不会立刻通过网络传输到下一个节点，当缓存写满，就持久化到本地硬盘上，当所有数据都被处理完成后，才开始将处理后的数据通过网络传输到下一个节点

- 流处理中，数据被一个节点处理完成后立即序列化化到缓存中传到下一个节点进行处理；而批处理在一个节点处理完后也会序列化到缓存中但会等到缓存写满并写入硬盘，所有数据处理完后才传到下一个节点

- 流处理对应低延迟的要求，批处理系统对应高吞吐量的要求

--3.2 Flink的执行引擎采用了一种十分灵活的方式，同时支持了上述两种数据传输模型

- 固定大小的缓存块 + 缓存块超时值

- Flink以固定的缓存块为单位进行网络数据传输，用户可以通过缓存块超时值指定缓存块的传输时机

- 缓存块的超时值为0，则Flink的数据传输方式类似上文所提到流处理系统的标准模型，此时系统可以获得最低的处理延迟

- 缓存块的超时值为无限大，则Flink的数据传输方式类似上文所提到批处理系统标准模型，此时系统可以获得最高的吞吐量

- 缓存块的超时值也可以设置为0到无限大之间的任意值。缓存块的超时阀值越小，则Flink流处理执行引擎的数据处理延迟越低，但吞吐量也会降低，反之亦然。通过调整缓存块的超时阀值，用户可根据需求灵活地权衡系统延迟和吞吐量

--3.3 备注：该部分主要参考[Flink 核心技术浅析（整理版）
](https://www.cnblogs.com/swordfall/p/10612404.html)

**4 Flink流处理的容错机制**

--4.1 Flink 提供的容错机制

- 分布式系统的容错：对于一个分布式系统来说，单个进程或是节点崩溃导致整个Job失败是经常发生的事情，在异常发生时不会丢失用户数据并能自动恢复才是分布式系统必须支持的特性之一

- 批处理系统的容错：由于文件可以重复访问，当个某个任务失败后，重启该任务即可

- Flink通过Checkpoint机制实现容错，该思想借鉴了Chandy和Lamport在1985年发表的一篇关于分布式快照的论文

--4.2 分布式快照Chandy Lamport算法逻辑(该部分主要参考[Flink 核心技术浅析（整理版）
之2.2 Flink流处理的容错机制](https://www.cnblogs.com/swordfall/p/10612404.html))

- 按照用户自定义的分布式快照间隔时间，Flink会定时在所有数据源中插入一种特殊的快照标记消息，这些快照标记消息和其他消息一样在DAG中流动，但是不会被用户定义的业务逻辑所处理，每一个快照标记消息都将其所在的数据流分成两部分：本次快照数据和下次快照数据

- 快照标记消息沿着DAG流经各个操作符，当操作符处理到快照标记消息时，会对自己的状态进行快照，并存储起来。当一个操作符有多个输入的时候，Flink会将先抵达的快照标记消息及其之后的消息缓存起来，当所有的输入中对应该快照的快照标记消息全部抵达后，操作符对自己的状态快照并存储，之后处理所有快照标记消息之后的已缓存消息。操作符对自己的状态快照并存储可以是异步与增量的操作，并不需要阻塞消息的处理

- 当所有的Data Sink（终点操作符）都收到快照标记信息并对自己的状态快照和存储后，整个分布式快照就完成了，同时通知数据源释放该快照标记消息之前的所有消息。若之后发生节点崩溃等异常情况时，只需要恢复之前存储的分布式快照状态，并从数据源重发该快照以后的消息就可以了

- 分布式快照的时间间隔越短，错误恢复的时间越少，与吞吐量负相关

--4.3 Flink Checkpoint机制具体实现流程(该部分主要参考：[追源索骥：透过源码看懂Flink核心框架的执行流程之5.1.4 Flink的分布式快照机制](https://www.cnblogs.com/bethunebtj/p/9168274.html#5-%E4%B8%BA%E6%89%A7%E8%A1%8C%E4%BF%9D%E9%A9%BE%E6%8A%A4%E8%88%AAfault-tolerant%E4%B8%8E%E4%BF%9D%E8%AF%81exactly-once%E8%AF%AD%E4%B9%89))

- fault tolerant就是从持久化存储中读取上次记录的这些元信息，并且恢复到程序中

- Flink引入了一个概念，叫做Barrier。Barrier是一种标记，它被source产生并且插入到流数据中，被发送到下游节点。当下游节点处理到该barrier标志时，这就意味着在该barrier插入到流数据时，已经进入系统的数据在当前节点已经被处理完毕

- 每当一个barrier流过一个算子节点时，就说明了在该算子上，可以触发一次检查点，用以保存当前节点的状态和已经处理过的数据，这就是一份快照

- 与此同时，该算子会向下游发送该barrier。因为数据在算子之间是按顺序发送的，所以当下游节点收到该barrier时，也就意味着同样的一批数据在下游节点上也处理完毕，可以进行一次checkpoint，保存基于该节点的一份快照，快照完成后，会通知JobMananger自己完成了这个快照

- 如果有不止一个下游节点，就向每个下游发送barrier

- 如果有不止一个上游节点，那么就要等到所有上游节点的同一批次的barrier到达之后，才能触发checkpoint。因为每个节点运算速度不同，所以有的上游节点可能已经在发下个barrier周期的数据了，有的上游节点还没发送本次的barrier，这时候，当前算子就要缓存一下提前到来的数据，等比较慢的上游节点发送barrier之后，才能处理下一批数据

- 当整个程序的最后一个算子sink都收到了这个barrier，也就意味着这个barrier和上个barrier之间所夹杂的这批元素已经全部落袋为安。这时，最后一个算子通知JobManager整个流程已经完成，而JobManager随后发出通知，要求所有算子删除本次快照内容，以完成清理

--4.4 流处理引擎提供的三种数据处理语义(该部分主要参考：[谈谈流计算中的『Exactly Once』特性](https://zhuanlan.zhihu.com/p/69958793))

- 流处理引擎提供的三种数据处理语义
   - 最多一次（At-most-once）：本质上是一『尽力而为』的方法。保证数据或事件最多由应用程序中的所有算子处理一次。 这意味着如果数据在被流应用程序完全处理之前发生丢失，则不会进行其他重试或者重新发送
   - 至少一次（At-least-once）：应用程序中的所有算子都保证数据或事件至少被处理一次。这通常意味着如果事件在流应用程序完全处理之前丢失，则将从源头重放或重新传输事件。然而，由于事件是可以被重传的，因此一个事件有时会被处理多次，这就是所谓的至少一次
   - 精确一次（Exactly-once）：即使是在各种故障的情况下，流应用程序中的所有算子都保证事件只会被『精确一次』的处理

- 对Exactly-once的理解
   - 事件的处理可以发生多次，但是该处理的效果只在持久后端状态存储中反映一次，流处理引擎管理的算子状态的不同更新只反映一次

--4.5 Flink通过两阶段提交协议提供端到端的Exactly-Once保证(该部分主要参考：[深入理解Flink ---- End-to-End Exactly-Once语义](https://www.cnblogs.com/tuowang/p/9025266.html))

- Exactly-Once语义是针对Flink系统内部而言的，结合Kafka如何构建端到端的Exactly-Once处理

- 两阶段提交协议
   - 第一阶段Phase 1，Pre-commit：Flink的JobManager向source注入checkpoint barrier以开启这次snapshot，barrier从source流向sink，每个进行snapshot的算子成功snapshot后,都会向JobManager发送ACK，当sink完成snapshot后, 向JobManager发送ACK的同时向kafka进行pre-commit
   - 第二阶段Phase 2，Commit：当JobManager接收到所有算子的ACK后,就会通知所有的算子这次checkpoint已经完成，Sink接收到这个通知后, 就向kafka进行commit,正式把数据写入到kafka

- 不同阶段错误的恢复
   - 在pre-commit前fail over, 系统恢复到最近的checkponit
   - 在pre-commit后,commit前fail over,系统恢复到刚完成pre-commit时的状态

- Flink的two phase commit实现：抽象类TwoPhaseCommitSinkFunction
   - beginTransaction()：开启事务.创建一个临时文件.后续把原要写入到外部系统的数据写入到这个临时文件
   - preCommit()：flush并close这个文件,之后便不再往其中写数据.同时开启一个新的事务供下个checkponit使用
   - commit()：把pre-committed的临时文件移动到指定目录
   - abort()：删除掉pre-committed的临时文件

--4.6 排查 Flink Checkpoint 超时问题(该部分主要参考：[Flink Checkpoint 问题排查实用指南](https://www.infoq.cn/article/G8YlV3i2aKmmzGCcz8kU))

- Source Trigger Checkpoint 慢
   - 这个一般发生较少，但是也有可能，因为 source 做 snapshot 并往下游发送 barrier 的时候，需要抢锁（。如果一直抢不到锁的话，则可能导致 Checkpoint 一直得不到机会进行。如果在 Source 所在的 taskmanager.log 中找不到开始做 Checkpoint 的 log，则可以考虑是否属于这种情况，可以通过 jstack 进行进一步确认锁的持有情况

- 使用增量 Checkpoint
   - Flink 中 Checkpoint 有两种模式，全量 Checkpoint 和 增量 Checkpoint，其中全量 Checkpoint 会把当前的 state 全部备份一次到持久化存储，而增量 Checkpoint，则只备份上一次 Checkpoint 中不存在的 state，因此增量 Checkpoint 每次上传的内容会相对更好，在速度上会有更大的优势

- 作业存在反压或者数据倾斜
   - task 仅在接受到所有的 barrier 之后才会进行 snapshot，如果作业存在反压，或者有数据倾斜，则会导致全部的 channel 或者某些 channel 的 barrier 发送慢，从而整体影响 Checkpoint 的时间，可以通过反压监控面板排查

- Barrier 对齐慢
   - Checkpoint 在 task 端分为 barrier 对齐（收齐所有上游发送过来的 barrier），然后开始同步阶段，再做异步阶段。如果 barrier 一直对不齐的话，就不会开始做 snapshot
   - barrier 对齐之后会有日志打印，如果 taskmanager.log 中没有这个日志，则表示 barrier 一直没有对齐，接下来我们需要了解哪些上游的 barrier 没有发送下来

- 主线程太忙，导致没机会做 snapshot
   - 在 task 端，所有的处理都是单线程的，数据处理和 barrier 处理都由主线程处理，如果主线程在处理太慢（比如使用 RocksDBBackend，state 操作慢导致整体处理慢），导致 barrier 处理的慢，也会影响整体 Checkpoint 的进度，在这一步我们需要能够查看某个 PID 对应 hotmethod，这里推荐两个方法：多次连续 jstack，查看一直处于 RUNNABLE 状态的线程有哪些；使用工具 AsyncProfile dump 一份火焰图，查看占用 CPU 最多的栈

**5 Flink流处理的时间窗口**

--5.1 Flink的时间模型

- EventTime是数据被生产出来的时间，可以是比如传感器发出信号的时间等（此时数据还没有被传输给flink）

- IngestionTime是数据进入flink的时间，也就是从Source进入flink流的时间（此时数据刚刚被传给flink）

- ProcessingTime是针对当前算子的系统时间，是指该数据已经进入某个operator时，operator所在系统的当前时间

--5.2 乱序问题

- 乱序问题一般是和EventTime关联的，对于一个流式处理系统的process time来说，是不存在乱序问题的。所以下面介绍的watermark/allowedLateness也只是在EventTime作为主时间才生效

- Flink通过watermark+window+trigger解决乱序问题
   - window解决的是where，也就是将无界数据划分成有界数据(window提供了allowedLateness方法，使得更大限度的允许乱序)
   - trigger用来设计窗口数据触发条件
   - watermark用来标记窗口的完整性

--5.3 watermark+window+trigger解决乱序问题

- watermark(水位线)
   - watermark是流式系统中主要用于解决流式系统中数据乱序问题的机制，方法是用于标记当前处理到什么水位的数据了，这意味着再早于这个水位的数据过来会被直接丢弃。这使得引擎可以自动跟踪数据中的当前事件时间，并尝试相应地清除旧状态
   - watermark和数据本身一样作为正常的消息在流中流动

- trigger
   - trigger指明在哪些条件下触发window计算，基于处理数据时的时间以及事件的特定属性。一般trigger的实现是当watermark处于某种时间条件下或者窗口数据达到一定条件，窗口的数据开始计算
   - 每次trigger对新增的数据相关的window进行重新计算，输出有complete, append,update三种输出模式
      - Complete mode(全量模式)：Result Table 全量输出，也就是重新计算过的window结果都输出。意味着这种模式下，每次读了新增的input数据，output的时候会把内存中resulttable中所有window的结果都输出一遍
      - Append mode (default)(增量模式)：只有 Result Table 中新增的行才会被输出，所谓新增是指自上一次 trigger 的时候。因为只是输出新增的行，所以如果老数据有改动就不适合使用这种模式。 更新的window并不输出，否则外存里的key就重了
      - Update mode(更新模式)：只要更新的 Row 都会被输出，相当于 Append mode 的加强版。而且是对外存中的相同key进行update，而不是append，需要外存是能kv操作的！只会输出新增和更新过的window的结果

- window的allowedLateness
   - allowedLateness就是针对event time而言，对于watermark超过end-of-window之后，还允许有一段时间（也是以event time来衡量）来等待之前的数据到达，以便再次处理这些数据
   - watermark是全局性的参数，用于管理消息的乱序，watermark超过window的endtime之后，就会触发窗口计算。一般情况下，触发窗口计算之后，窗口就销毁掉了，后面再来的数据也不会再计算，对于trigger是默认的EventTimeTrigger的情况下，allowedLateness会再次触发窗口的计算，而之前触发的数据，会buffer起来，直到watermark超过end-of-window + allowedLateness的时间，窗口的数据及元数据信息才会被删除
   - 比如window的endtime是5000，allowedLateness=0，那么如果watermark 5000到来之后，这个window就应该被清除。但是如果allowedLateness = 1000，则需要等water 6000(endtime + allowedLateness)到来之后，这个window才会被清掉

--5.4 watermark的生成和传递

- watermark的生成
   - Flink通过水位线分配器（TimestampsAndPeriodicWatermarksOperator和TimestampsAndPunctuatedWatermarksOperator这两个算子）向事件流中注入水位线。元素在streaming dataflow引擎中流动到WindowOperator时，会被分为两拨，分别是普通事件和水位线

- watermark的传递
   - 算子接受到的水印是来自其数据上游所有watermark的最小值
   - watermark以广播的形式在算子之间传播，当一个算子收到watermark时都要
      - 更新算子时间
      - 遍历计时器队列触发回调
      - 将watermark发送到下游

--5.5 备注：该部分主要参考[[源码分析] 从源码入手看 Flink Watermark 之传播过程](https://www.cnblogs.com/rossiXYZ/p/12345969.html)

**6 Flink处理反压问题**

--6.1 Storm、Spark  Streaming、Flink 提供的反压机制比较(该部分主要参考[流控-背压](https://wrm128.github.io/2019/11/29/%E6%B5%81%E6%8E%A7-%E8%83%8C%E5%8E%8B/)、[Flink 原理与实现：如何处理反压问题](http://wuchong.me/blog/2016/04/26/flink-internals-how-to-handle-backpressure/))

- 反压（Backpressure）:消费者需要多少，生产者就生产多少

- Storm
   - 自动反压机制(Automatic Back Pressure)通过监控Bolt中的接收队列的负载情况，如果超过高水位就会将反压信息写入zk，zk上的watch就会通知该拓扑的所有worker进入反压，最后Spout停止发送数据，直到接收队列的负载降到低水位以下
   - 存在的问题就是， 当下游出现阻塞时， 上游停止发送， 下游消除阻塞后，上游又开闸放水，过了一会儿，下游又阻塞，上游又限流， 如此反复， 整个数据流一直处在一个颠簸状态
   - Jstorm的改进方案是逐级降速与放水来进行反压；另外，Jstorm没有引入zk，而是通过TopologyMaster来协调拓扑进行反压

- Spark  Streaming
   - Spark Streaming程序中当计算过程中出现batch processing time > batch interval的情况时，(其中batch processing time为实际计算一个批次花费时间，batch interval为Streaming应用设置的批处理间隔),意味着处理数据的速度小于接收数据的速度，如果这种情况持续过长的时间，会造成数据在内存中堆积，导致Receiver所在Executor内存溢出等问题
   - Spark Streaming Backpressure: 根据JobScheduler反馈作业的执行信息来动态调整Receiver数据接收率。通过属性”spark.streaming.backpressure.enabled”来控制是否启用backpressure机制，默认值false，即不启用

- Flink
   - 固定大小缓冲池就像阻塞队列一样，保证了Flink有一套健壮的反压机制，使得 Task 生产数据的速度不会快于消费的速度。该方案可以从两个 Task 之间的数据传输自然地扩展到更复杂的 pipeline 中，保证反压机制可以扩散到整个 pipeline
   - 由Flink的网络传输中的内存管理来分析它的反压机制
      - 每个 Task 都包括了输入IG和输入RS，输入和输出的数据存在 Buffer 中（都是字节数据），网络上传输的数据会写到 Task 的 InputGate（IG）中，经过Task的处理后，再由Task写到 ResultPartition（RS）中
      - NetworkEnvironment 和 NetworkBufferPool 是 Task 之间共享的，每个 TM 只会实例化一个。TaskManager（TM）在启动时，会先初始化NetworkEnvironment对象，TM 中所有与网络相关的东西都由该类来管理（如 Netty 连接），其中就包括NetworkBufferPool
      - Task 线程启动时，会向 NetworkEnvironment 注册，NetworkEnvironment 会为 Task 的 InputGate（IG）和 ResultPartition（RP） 分别创建一个 LocalBufferPool（缓冲池）并设置可申请的 MemorySegment（内存块）数量。每当创建或销毁缓冲池时，NetworkBufferPool 会计算剩余空闲的内存块数量，并平均分配给已创建的缓冲池。注意，这个过程只是指定了缓冲池所能使用的内存块数量，并没有真正分配内存块，只有当需要时才分配。在 Task 线程执行过程中，当 Netty 接收端收到数据时，为了将 Netty 中的数据拷贝到 Task 中，InputChannel（实际是 RemoteInputChannel）会向其对应的缓冲池申请内存块（上图中的①）。如果缓冲池中也没有可用的内存块且已申请的数量还没到池子上限，则会向 NetworkBufferPool 申请内存块（上图中的②）并交给 InputChannel 填上数据（上图中的③和④）。如果缓冲池已申请的数量达到上限了呢？或者 NetworkBufferPool 也没有可用内存块了呢？这时候，Task 的 Netty Channel 会暂停读取，上游的发送端会立即响应停止发送，拓扑会进入反压状态。当 Task 线程写数据到 ResultPartition 时，也会向缓冲池请求内存块，如果没有可用内存块时，会阻塞在请求内存块的地方，达到暂停写入的目的。当一个内存块被消费完成之后（在输入端是指内存块中的字节被反序列化成对象了，在输出端是指内存块中的字节写入到 Netty Channel 了），会调用 Buffer.recycle() 方法，会将内存块还给 LocalBufferPool （上图中的⑤）。如果LocalBufferPool中当前申请的数量超过了池子容量（由于上文提到的动态容量，由于新注册的 Task 导致该池子容量变小），则LocalBufferPool会将该内存块回收给 NetworkBufferPool（上图中的⑥）。如果没超过池子容量，则会继续留在池子中，减少反复申请的开销
   - 本地传输：如果 Task 1 和 Task 2 运行在同一个 worker 节点（TaskManager），该 buffer 可以直接交给下一个 Task。一旦 Task 2 消费了该 buffer，则该 buffer 会被缓冲池1回收。如果 Task 2 的速度比 1 慢，那么 buffer 回收的速度就会赶不上 Task 1 取 buffer 的速度，导致缓冲池1无可用的 buffer，Task 1 等待在可用的 buffer 上。最终形成 Task 1 的降速
   - 远程传输：如果 Task 1 和 Task 2 运行在不同的 worker 节点上，那么 buffer 会在发送到网络（TCP Channel）后被回收。在接收端，会从 LocalBufferPool 中申请 buffer，然后拷贝网络中的数据到 buffer 中。如果没有可用的 buffer，会停止从 TCP 连接中读取数据。在输出端，通过 Netty 的水位值机制来保证不往网络中写入太多数据（后面会说）。如果网络中的数据（Netty输出缓冲中的字节数）超过了高水位值，我们会等到其降到低水位值以下才继续写入数据。这保证了网络中不会有太多的数据。如果接收端停止消费网络中的数据（由于接收端缓冲池没有可用 buffer），网络中的缓冲数据就会堆积，那么发送端也会暂停发送。另外，这会使得发送端的缓冲池得不到回收，writer 阻塞在向 LocalBufferPool 请求 buffer，阻塞了 writer 往 ResultSubPartition 写数据

   - Netty 水位值机制
      - 当输出缓冲中的字节数超过了高水位值, 则 Channel.isWritable() 会返回false。当输出缓存中的字节数又掉到了低水位值以下, 则 Channel.isWritable() 会重新返回true

--6.2 排查反压瓶颈，处理反压问题(该部分主要参考[如何分析及处理 Flink 反压](http://www.whitewood.me/2019/11/03/Flink-%E5%8F%8D%E5%8E%8B%E5%88%86%E6%9E%90%E5%8F%8A%E5%A4%84%E7%90%86/))

- 反压的影响
   - 反压会影响到两项指标: checkpoint 时长和 state 大小
   - checkpoint barrier 是不会越过普通数据的，数据处理被阻塞也会导致 checkpoint barrier 流经整个数据管道的时长变长，checkpoint 时间变长有可能导致 checkpoint 超时失败
   - 对于有两个以上输入管道的 Operator，checkpoint barrier 需要对齐（Alignment），接受到较快的输入管道的 barrier 后，它后面数据会被缓存起来但不处理，直到较慢的输入管道的 barrier 也到达，这些被缓存的数据会被放到state 里面， state 大小同样可能拖慢 checkpoint 甚至导致 OOM （使用 Heap-based StateBackend）或者物理内存使用超出容器资源（使用 RocksDBStateBackend）的稳定性问题

- 处理反压问题步骤
   - 1.定位反压节点；2.分析具体原因及处理
      - 定位反压可以从 Web UI 的反压监控面板和 Task Metric 两者入手，前者方便简单分析，后者适合深入挖掘。定位到反压节点后我们可以通过数据分布、CPU Profile 和 GC 指标日志等手段来进一步分析反压背后的具体原因并进行针对性的优化
   - 定位到造成反压的节点，这主要有两种办法: 1.通过 Flink Web UI 自带的反压监控面板；2.通过 Flink Task Metrics
      - Flink Web UI 的反压监控提供了 SubTask 级别的反压监控，原理是通过周期性对 Task 线程的栈信息采样，计算线程请求buffer的阻塞率 = 线程被阻塞在请求 Buffer（意味着被下游队列阻塞）的频率来判断该节点是否处于反压状态。默认配置下，这个频率在 0.1 以下则为 OK，0.1 至 0.5 为 LOW，而超过 0.5 则为 HIGH
         - 处于反压状态，那么有两种可能性：该节点的发送速率跟不上它的产生数据速率，该节点则为反压的根源节点；下游的节点接受速率较慢，通过反压机制限制了该节点的发送速率，需要排查下游节点
      - Task Metrics
         - 以下是几个 Metrics:
         
         | Metris | 描述 |
         | ---- | --- |
         | outPoolUsage|发送端 Buffer 的使用率|
         | inPoolUsage|接收端 Buffer 的使用率 |
         | floatingBuffersUsage（1.9 以上）|接收端 Floating Buffer 的使用率 |
         | exclusiveBuffersUsage （1.9 以上）|接收端 Exclusive Buffer 的使用率 |
    
         - outPoolUsage 和 inPoolUsage 同为低或同为高分别表明当前 Subtask 正常或处于被下游反压，这应该没有太多疑问。而比较有趣的是当 outPoolUsage 和 inPoolUsage 表现不同时，这可能是出于反压传导的中间状态或者表明该 Subtask 就是反压的根源。如果一个 Subtask 的 outPoolUsage 是高，通常是被下游 Task 所影响，所以可以排查它本身是反压根源的可能性。如果一个 Subtask 的 outPoolUsage 是低，但其 inPoolUsage 是高，则表明它有可能是反压的根源。因为通常反压会传导至其上游，导致上游某些 Subtask 的 outPoolUsage 为高
   - 分析具体原因及处理
      - 数据分布造成：很多情况下的反压是由于数据倾斜造成的，这点我们可以通过 Web UI 各个 SubTask 的 Records Sent 和 Record Received 来确认，另外 Checkpoint detail 里不同 SubTask 的 State size 也是一个分析数据倾斜的有用指标
      - CPU性能：最常见的问题可能是用户代码的执行效率问题（频繁被阻塞或者性能问题）。最有用的办法就是对 TaskManager 进行 CPU profile，从中我们可以分析到 Task Thread 是否跑满一个 CPU 核：如果是的话要分析 CPU 主要花费在哪些函数里面，比如我们生产环境中就偶尔遇到卡在 Regex 的用户函数（ReDoS）；如果不是的话要看 Task Thread 阻塞在哪里，可能是用户函数本身有些同步的调用，可能是 checkpoint 或者 GC 等系统活动导致的暂时系统暂停。当然，性能分析的结果也可能是正常的，只是作业申请的资源不足而导致了反压，这就通常要求拓展并行度
      - GC：TaskManager 的内存以及 GC 问题也可能会导致反压，包括 TaskManager JVM 各区内存不合理导致的频繁 Full GC 甚至失联。推荐可以通过给 TaskManager 启用 G1 垃圾回收器来优化 GC，并加上 -XX:+PrintGCDetails 来打印 GC 日志的方式来观察 GC 的问题

--6.3 处理 Flink 作业中的数据倾斜问题(该部分主要参考[Flink零基础教程：并行度和数据重分布](https://juejin.im/post/5e056db8518825124953fecf))

- 数据倾斜就是我们在计算数据的时候，数据的分散度不够，导致大量的数据集中到了一台或者几台机器上计算，这些数据的计算速度远远低于平均计算速度，导致整个计算过程过慢

- 数据重分布：默认情况下，数据是自动分配到多个实例上的。有的时候，我们需要手动对数据在多个实例上进行分配，例如，我们知道某个实例上的数据过多，其他实例上的数据稀疏，产生了数据倾斜，这时我们需要将数据均匀分布到各个实例上，以避免部分实例负载过重。数据倾斜问题会导致整个作业的计算时间过长或者内存不足等问题

- 下文涉及到的各个数据重分布算子的输入是DataStream，输出也是DataStream。keyBy也有对数据进行分组和数据重分布的功能，但keyBy输出的是KeyedStream
   - shuffle
      - shuffle基于正态分布，将数据随机分配到下游各算子实例上
      ~~~
        dataStream.shuffle()
      ~~~
   - rebalance
      - rebalance使用Round-ribon思想将数据均匀分配到各实例上。Round-ribon是负载均衡领域经常使用的均匀分配的方法，上游的数据会轮询式地分配到下游的所有的实例上
      - Round-ribon参考[为什么轮询调度算法称为 Round Robin](https://zhuanlan.zhihu.com/p/84799744)
      ~~~
        dataStream.rebalance()
      ~~~
   - rescale
      - rescale与rebalance很像，也是将数据均匀分布到各下游各实例上，但它的传输开销更小，因为rescale并不是将每个数据轮询地发送给下游每个实例，而是就近发送给下游实例
      ~~~
        dataStream.rescale()
      ~~~
   - broadcast
      - broadcast翻译过来为广播，在Flink里，数据会被复制并广播发送给下游的所有实例上
      ~~~
        dataStream.broadcast()
      ~~~
   - partitionCustom
      - 以使用partitionCustom来自定义数据重分布逻辑。partitionCustom有两个参数：第一个参数是自定义的Partitioner，我们需要重写里面的partition函数；第二个参数是对数据流哪个字段使用partiton逻辑。partition函数的返回一个整数，表示该元素将被路由到下游第几个实例
      ~~~
        package com.flink.tutorials.api.transformations

        import org.apache.flink.api.common.functions.Partitioner
        import org.apache.flink.streaming.api.scala._

        object PartitionCustomExample {

        /**
            * Partitioner[T] 其中泛型T为指定的字段类型
            * 重写partiton函数，并根据T字段对数据流中的所有元素进行数据重分配
            * */
        class MyPartitioner extends Partitioner[String] {

            val rand = scala.util.Random

            /**
            * key 泛型T 即根据哪个字段进行数据重分配，本例中是(Int, String)中的String
            * numPartitons 为当前有多少个并行实例
            * 函数返回值是一个Int 为该元素将被发送给下游第几个实例
            * */
            override def partition(key: String, numPartitions: Int): Int = {
            var randomNum = rand.nextInt(numPartitions / 2)

            // 如果字符串中包含数字，该元素将被路由到前半部分，否则将被路由到后半部分。
            if (key.exists(_.isDigit)) {
                return randomNum
            } else {
                return randomNum + numPartitions / 2
            }
            }
        }

        def main(args: Array[String]): Unit = {

            val senv: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment

            // 获取当前执行环境的默认并行度
            val defaultParalleism = senv.getParallelism

            // 设置所有算子的并行度为4，表示所有算子的并行执行的实例数为4
            senv.setParallelism(4)

            val dataStream: DataStream[(Int, String)] = senv.fromElements((1, "123"), (2, "abc"), (3, "256"), (4, "zyx")
            , (5, "bcd"), (6, "666"))



            // 对(Int, String)中的第二个字段使用 MyPartitioner 中的重分布逻辑
            val partitioned = dataStream.partitionCustom(new MyPartitioner, 1)

            partitioned.print()

            senv.execute("partition custom transformation")

        }

        }

      ~~~


**7 Flink重启策略**

--7.1 默认重启策略

- Flink支持不同的重启策略，以在故障发生时控制作业如何重启

- 默认重启策略是通过Flink的配置文件设置的flink-conf.yaml。定义策略的配置key为: restart-strategy。如果未启用检查点，则使用“无重启”策略。如果激活了检查点但未配置重启策略，则使用“固定延迟策略”：restart-strategy.fixed-delay.attempts: Integer.MAX_VALUE尝试重启(会造成Flink作业频繁重启)

- 重启策略可以在flink-conf.yaml中配置，表示全局的配置。也可以在应用代码中动态指定，会覆盖全局配置

--7.2 重启策略

- 固定间隔 (Fixed delay)
   - 全局配置 flink-conf.yaml
   ~~~
    restart-strategy: fixed-delay 
    restart-strategy.fixed-delay.attempts: 3 
    restart-strategy.fixed-delay.delay: 10 s
   ~~~
   - 应用代码设置
   ~~~
   env.setRestartStrategy(RestartStrategies.fixedDelayRestart( 3,// 尝试重启的次数 
    Time.of(10, TimeUnit.SECONDS) // 间隔 ));
   ~~~

- 失败率 (Failure rate)
   - 失败率重启策略在Job失败后会重启，但是超过失败率后，Job会最终被认定失败。在两个连续的重启尝试之间，重启策略会等待一个固定的时间。下面配置是5分钟内若失败了3次则认为该job失败，重试间隔为10s
   - 全局配置 flink-conf.yaml
   ~~~
    restart-strategy: failure-rate  
    restart-strategy.failure-rate.max-failures-per-interval: 3  
    restart-strategy.failure-rate.failure-rate-interval: 5 min  
    restart-strategy.failure-rate.delay: 10 s
   ~~~
   - 应用代码设置
   ~~~
   env.setRestartStrategy(RestartStrategies.failureRateRestart(  3,//一个时间段内的最大失败次数 
   Time.of(5, TimeUnit.MINUTES), // 衡量失败次数的是时间段  
   Time.of(10, TimeUnit.SECONDS) // 间隔  ));
   ~~~

- 无重启 (No restart)
   - 全局配置 flink-conf.yaml
   ~~~
    restart-strategy: none
   ~~~
   - 应用代码设置
   ~~~
   ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();     
   env.setRestartStrategy(RestartStrategies.noRestart());
   ~~~

--7.3 参考
- [Flink重启策略](https://yq.aliyun.com/articles/709993)
- [Flink 重启策略](https://www.jianshu.com/p/22409ccc7905)

**8 Flink的算子**

--8.1 Join(该部分参考：[Apache-Flink深度解析-JOIN 算子](https://cloud.tencent.com/developer/article/1407308))

- 为什么需要Join
   - JOIN的本质是数据拼接，因为所有数据列无法存储在一张大表中
   - 数据库范式
      - 1NF - 列不可再分
      - 2NF - 符合1NF，并且非主键属性全部依赖于主键属性
      - 3NF - 符合2NF，并且消除传递依赖，即：任何字段不能由其他字段派生出来
      - BCNF - 符合3NF，并且主键属性之间无依赖关系

- Join种类
   - CROSS JOIN - 交叉连接，计算笛卡儿积
   - INNER JOIN - 内连接，返回满足条件的记录
   - OUTER JOIN
      - LEFT - 返回左表所有行，右表不存在补NULL
      - RIGHT - 返回右表所有行，左边不存在补NULL
      - FULL -  返回左表和右表的并集，不存在一边补NULL
   - SELF JOIN - 自连接，将表查询时候命名不同的别名

- Flink双流Join
   - 支持Join种类

   |框架|CROSS|INNER|OUTER|SELF|ON链接条件|WHERE过滤条件|
   |--|--|--|--|--|--|--|
   |Apache Flink|N|Y|Y|Y|必须|可选|

   - 双流JOIN与传统数据库表JOIN的区别
      - 左右两边的数据集合无穷 - 传统数据库左右两个表的数据集合是有限的，双流JOIN的数据会源源不断的流入
      - JOIN的结果不断产生/更新 - 传统数据库表JOIN是一次执行产生最终结果后退出，双流JOIN会持续不断的产生新的结果
      - 双流JOIN由于左右两边的流的速度不一样，会导致左边数据到来的时候右边数据还没有到来，或者右边数据到来的时候左边数据没有到来，所以在实现中要将左右两边的流数据进行保存，以保证JOIN的语义。在Blink中会以State的方式进行数据的存储
   - 数据的保存
      - 不论是INNER JOIN还是OUTER JOIN 都需要对左右两边的流的数据进行保存，JOIN算子会开辟左右两个State进行数据存储
   - INNER JOIN实现
      - INNER JOIN只有符合JOIN条件时候才会有JOIN结果流出到下游，比如右边最先来的1，2，3个事件，流入时候没有任何输出，因为左边还没有可以JOIN的事
      - INNER JOIN两边的数据不论如何乱序，都能够保证和传统数据库语义一致，因为我们保存了左右两个流的所有事件到state中
   - LEFT OUTER JOIN 实现(其他OUTER JOIN同理)
      - LEFT OUTER JOIN 可以简写 LEFT JOIN，语义上和INNER JOIN的区别是不论右流是否有JOIN的事件，左流的事件都需要流入下游节点，但右流没有可以JION的事件时候，右边的事件补NULL

--8.2 Flink的其他算子

- 参考：[Flink 从 0 到 1 学习 —— Flink Data transformation(转换)](http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/)、[Flink DataStream 算子 Map、FlatMap、Filter、KeyBy、Reduce、Fold、Aggregate](https://blog.csdn.net/wangpei1949/article/details/101625394)

8.2.1 Map：一对一

8.2.2 FlatMap：一对0，1，多

8.2.3 Filter：函数根据条件判断出结果

8.2.4 KeyBy：在逻辑上是基于 key 对流进行分区。在内部，它使用 hash 函数对流进行分区。它返回 KeyedDataStream 数据流

8.2.5 Reduce：返回单个的结果值，常用的方法有 average, sum, min, max, count，使用 reduce 方法都可实现

8.2.6 Fold：基于初始值和FoldFunction进行滚动折叠(Fold)，并向下游算子输出每次滚动折叠后的结果
~~~
package com.bigdata.flink.dataStreamFoldOperator;

import com.bigdata.flink.beans.UserAction;
import org.apache.flink.api.common.functions.FoldFunction;
import org.apache.flink.api.java.functions.KeySelector;
import org.apache.flink.streaming.api.datastream.DataStreamSource;
import org.apache.flink.streaming.api.datastream.KeyedStream;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;

import java.util.Arrays;

/**
 * Summary:
 *      Fold: 基于初始值和自定义的FoldFunction滚动折叠后发出新值
 */
public class DataStreamFoldOperator {
    public static void main(String[] args) throws Exception{

        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        // 输入: 用户行为。某个用户在某个时刻点击或浏览了某个商品，以及商品的价格。
        DataStreamSource<UserAction> source = env.fromCollection(Arrays.asList(
                new UserAction("userID1", 1293984000, "click", "productID1", 10),
                new UserAction("userID2", 1293984001, "browse", "productID2", 8),
                new UserAction("userID2", 1293984002, "browse", "productID2", 8),
                new UserAction("userID2", 1293984003, "browse", "productID2", 8),
                new UserAction("userID1", 1293984002, "click", "productID1", 10),
                new UserAction("userID1", 1293984003, "click", "productID3", 10),
                new UserAction("userID1", 1293984004, "click", "productID1", 10)
        ));

        // 转换: KeyBy对数据重分区
        KeyedStream<UserAction, String> keyedStream = source.keyBy(new KeySelector<UserAction, String>() {
            @Override
            public String getKey(UserAction value) throws Exception {
                return value.getUserID();
            }
        });

        // 转换: Fold 基于初始值和FoldFunction滚动折叠
        SingleOutputStreamOperator<String> result = keyedStream.fold("浏览的商品及价格:", new FoldFunction<UserAction, String>() {
            @Override
            public String fold(String accumulator, UserAction value) throws Exception {
                if(accumulator.startsWith("userID")){
                    return accumulator + " -> " + value.getProductID()+":"+value.getProductPrice();
                }else {
                    return value.getUserID()+" " +accumulator + " -> " + value.getProductID()+":"+value.getProductPrice();
                }
            }
        });

        // 输出: 输出到控制台
        // 每一条数据都会触发计算并输出
        // userID1 浏览的商品及价格: -> productID1:10
        // userID1 浏览的商品及价格: -> productID1:10 -> productID1:10
        // userID1 浏览的商品及价格: -> productID1:10 -> productID1:10 -> productID3:10
        // userID1 浏览的商品及价格: -> productID1:10 -> productID1:10 -> productID3:10 -> productID1:10
        // userID2 浏览的商品及价格: -> productID2:8
        // userID2 浏览的商品及价格: -> productID2:8 -> productID2:8
        // userID2 浏览的商品及价格: -> productID2:8 -> productID2:8 -> productID2:8
        result.print();

        env.execute();

    }
}
~~~

8.2.7 AAggregate：对KeyedStream按指定字段滚动聚合并输出每一次滚动聚合后的结果。默认的聚合函数有:sum、min、minBy、max、mabBy
   - max(field)与maxBy(field)的区别: maxBy返回field最大的那条数据;而max则是将最大的field的值赋值给第一条数据并返回第一条数据。同理,min与minBy

8.2.8 Window：函数允许按时间或其他条件对现有 KeyedStream 进行分组

8.2.9 Union：函数将两个或多个数据流结合在一起。 这样就可以并行地组合数据流。 如果我们将一个流与自身组合，那么它会输出每个记录两次

8.2.10 Split：此功能根据条件将流拆分为两个或多个流。 当您获得混合流并且您可能希望单独处理每个数据流时，可以使用此方法

8.2.11 Select：此功能允许您从拆分流中选择特定流

8.2.12 Project：Project 函数允许您从事件流中选择属性子集，并仅将所选元素发送到下一个处理流

**9 Flink与Spark Streaming的比较**

-- 9.1 比较列表
|比较点|Spark Streaming|Flink|
|--|--|--|
|流式引擎|Spark的数据处理基于分布式弹性数据集RDD。这个内存数据结构使得spark可以通过固定内存做大批量计算。Spark Streaming是通过将数据流转成小批（micro-batches），即收集一段时间（time-window）内到达的所有数据，并在其上进行常规批处理，所以严格意义上，它不是流|固定大小的缓存块 + 缓存块超时值，缓存块的超时值为0时，Flink的数据传输方式类似流处理系统的标准模型，此时系统可以获得最低的处理延迟|
|延迟|秒级(基于数据块)|毫秒级(基于每条记录)|
|容错|Spark Streaming可以恢复失败的任务，并且无需额外的代码或配置即可提供Exactly-oncy的语义。哪个节点失败调度给其他节点执行，这部分失败数据可取自其他节点的replica或者尝试从source重读|基于checkpoint检查点机制|
|反压|Spark的micro-batch模型导致了它需要单独引入反压机制，通过估计当前系统处理数据的速率，调节系统接受数据的速率来与之相匹配|Flink支持反压但无需单独引入反压机制，因为系统接收数据的速率和处理数据的速率是自然匹配的。系统接收数据的前提是接收数据的Task必须有空闲可用的Buffer，该数据被继续处理的前提是下游Task也有空闲可用的Buffer|
|Exactly Once|Spark 在进行Streaming的时候将micro-Batches分配到多个节点运行，每个Micro-Batch可以成功运行或者发生故障，当发生故障时，那个对应的Micro-Batch只要简单地重新计算即可，因为它是持久化并且无状态的，所以要保证Exactly-Once这种投递方式也是很简单的|基于checkpoint检查点机制，状态的提交或对事实来源的持久后端进行的更新可描述为事件（Occurring）的严格一次|

-- 9.2 本部分参考

- [流式技术哪家强：Spark vs Flink vs Storm](https://tianchi.aliyun.com/forum/postDetail?postId=7612)
- [Flink，Storm，Spark Streaming三种流框架的对比分析](https://bigdata.163yun.com/product/article/5)


**参考**

- [Flink 面试通关手册](https://gitbook.cn/books/5de5ea4c984ce511296b746a/index.html)
- [Flink面试题目](https://mp.weixin.qq.com/s/Nf4iV_sNitOMQkw7PGgvOQ)
- [Flink 核心技术浅析（整理版）](https://www.cnblogs.com/swordfall/p/10612404.html)
- [Cris 带你快速入门 Flink](https://juejin.im/post/5c4f16dbe51d454f342fb7e7#heading-49)
- [深入理解Apache Flink核心技术](https://www.cnblogs.com/feiyudemeng/p/8998772.html)
- [追源索骥：透过源码看懂Flink核心框架的执行流程](https://www.cnblogs.com/bethunebtj/p/9168274.html#71-eventtime%E6%97%B6%E9%97%B4%E6%A8%A1%E5%9E%8B)
- [Apache Flink 是什么？](https://flink.apache.org/zh/flink-architecture.html)
- [Flink 原理与实现：理解 Flink 中的计算资源](http://wuchong.me/blog/2016/05/09/flink-internals-understanding-execution-resources/)
- [Flink--对parallelism 和 slot的理解](https://www.jianshu.com/p/b58988bcfb48)
- [Flink Checkpoint 问题排查实用指南](https://www.infoq.cn/article/G8YlV3i2aKmmzGCcz8kU)
- [谈谈流计算中的『Exactly Once』特性](https://zhuanlan.zhihu.com/p/69958793)
- [Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理](https://segmentfault.com/a/1190000019549398)
- [深入理解Flink ---- End-to-End Exactly-Once语义](https://www.cnblogs.com/tuowang/p/9025266.html)
- [分布式快照算法: Chandy-Lamport 算法](https://zhuanlan.zhihu.com/p/53482103)
- [[源码分析] 从源码入手看 Flink Watermark 之传播过程](https://www.cnblogs.com/rossiXYZ/p/12345969.html)
- [Flink 原理与实现：如何处理反压问题](http://wuchong.me/blog/2016/04/26/flink-internals-how-to-handle-backpressure/)
- [如何分析及处理 Flink 反压](http://www.whitewood.me/2019/11/03/Flink-%E5%8F%8D%E5%8E%8B%E5%88%86%E6%9E%90%E5%8F%8A%E5%A4%84%E7%90%86/)
- [流控-背压](https://wrm128.github.io/2019/11/29/%E6%B5%81%E6%8E%A7-%E8%83%8C%E5%8E%8B/)
- [为什么轮询调度算法称为 Round Robin ？](https://zhuanlan.zhihu.com/p/84799744)
- [Flink零基础教程：并行度和数据重分布](https://juejin.im/post/5e056db8518825124953fecf)
- [大数据常见问题之数据倾斜](https://blog.csdn.net/u010039929/article/details/55044407)
- [流式技术哪家强：Spark vs Flink vs Storm](https://tianchi.aliyun.com/forum/postDetail?postId=7612)
- [Flink，Storm，Spark Streaming三种流框架的对比分析](https://bigdata.163yun.com/product/article/5)