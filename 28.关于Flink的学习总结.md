# 关于Flink的学习总结

**1 Flink的概念**

--1.1 基本概念

- 分布式

- 支持流处理和批处理

- 开源计算平台

--1.2 特点

- 统一的批处理和流处理系统

- Flink流处理的容错机制

- Flink流处理的时间窗口

**2 Flink的架构**

--2.1 基本架构

2.1.1 Flink包含了两种类型的处理器

- JobManager(Master)
   - 负责接收 flink 的作业，调度task，协调检查点，协调失败时恢复等
   - 收集 job 的状态、管理 TaskManagers
   - Flink运行时至少存在一个master处理器，如果配置高可用模式则会存在多个master处理器，它们其中有一个是leader，其他的都是standby

- TaskManager(Worker)
   - Flink中资源管理的基本组件，是所有执行任务的基本容器，提供了内存管理、IO管理、通信管理等一系列功能
   - MemoryManager Flink并没有把所有内存的管理都委托给JVM，因为JVM普遍存在着存储对象密度低、大内存时GC对系统影响大等问题。所以Flink自己抽象了一套内存管理机制，将所有对象序列化后放在自己的MemorySegment上进行管理
   - IOManager flink通过IOManager管理磁盘IO的过程，提供了同步和异步两种写模式，又进一步区分了block、buffer和bulk三种读写方式
   - NetworkEnvironment 是TaskManager的网络 IO 组件，包含了追踪中间结果和数据交换的数据结构。它的构造器会统一将配置的内存先分配出来，抽象成 NetworkBufferPool 统一管理内存的申请和释放
   - Flink运行时至少会存在一个worker处理器

2.1.2 Flink中计算资源的介绍

- TaskManager：Flink中资源管理的基本组件，是所有执行任务的基本容器
   - 每个TaskManager都是一个独立的JVM进程

- TaskManager可以分为一个或者多个Task Slot：每个Task Slot代表了TaskManager的一个固定大小的资源子集。例如，一个拥有3个slot的 TaskManager，会将其管理的内存平均分成三分分给各个slot。将资源slot化意味着来自不同job的task不会为了内存而竞争，而是每个task都拥有一定数量的内存储备。
   - 同一JVM(TaskManager)中的任务(Task Slot)共享TCP连接和心跳消息
   - TaskManager的一个Slot代表一个可用线程，该线程具有固定的内存，注意 Slot 只对内存隔离，没有对CPU隔离
   - 每个slot能运行一个或多个task(一个task是一个线程，task是执行计算的最小结构)
   - 每个TaskManager的Task Slot数目代表了该TaskManager的并发能力，任务的并发能力由所有TaskManager的Task Slot数目决定。有两个 Task Manager，每个 TaskManager有三个slot，这样我们的算子最大并行度那么就可以达到6个
   - slot 的数量通常与每个TaskManager的可用CPU内核数成比例。一般情况下你的slot数是每个TaskManager的cpu的核数
   - parallelism与slot的区别，参考[Flink--对parallelism 和 slot的理解](https://www.jianshu.com/p/b58988bcfb48)：slot是指taskmanager的并发执行能力；parallelism是指 taskmanager 实际使用的并发能力(并行度)，一个特定operator的subtask的个数被称之为其parallelism(并行度)；taskmanager.numberOfTaskSlots:3；即每一个 taskmanager 中的分配3个TaskSlot, 3个 taskmanager 一共有 9 个 TaskSlot。parallelism.default:1；即运行程序默认的并行度为 1，9 个 TaskSlot 只用了 1 个，有 8 个空闲。设置合适的并行度才能提高效率

- Task Slot允许多个task共享：每个Task Slot能运行一个或多个task，为了资源更充分的利用，Flink提出了SlotSharingGroup，尽可能地让多个task共享一个slot
   - 条件：它们都来自同一个Job的不同task的subtask(同一job的一个task下的相同subtask应该并行，不能放在同一个slot中)
   - slot共享有以下两点好处：1.Flink集群所需的task slots数与job中最高的并行度一致。也就是说我们不需要再去计算一个程序总共会起多少个task了；2.更容易获得更充分的资源利用。如果没有slot共享，那么非密集型操作source/flatmap就会占用同密集型操作 keyAggregation/sink 一样多的资源。如果有slot共享，将基线的2个并行度增加到6个，能充分利用slot资源，同时保证每个TaskManager能平均分配到重的subtasks(该部分看参考链接的图)

- 算子operators的Chaining：Flink提出了Chaining，尽可能地将operators chain在一起作为一个task来处理
   - Flink会尽可能地将operator的subtask链接（chain）在一起形成task。每个task在一个线程中执行。
   - 好处：将operators链接成task是非常有效的优化，它能减少线程之间的切换，减少消息的序列化/反序列化，减少数据在缓冲区的交换，减少了延迟的同时提高整体的吞吐量
   - 不是任意两个operator就能chain一起的，其条件还是很苛刻的(具体条件可看链接)
   - 出于分布式执行的目的，Flink将operator的subtask链接在一起形成task，每个task在一个线程中执行。将operators链接成task是非常有效的优化：它能减少线程之间的切换和基于缓存区的数据交换，在减少时延的同时提升吞吐量。链接的行为可以在编程API中进行指定


--2.2 运行架构

2.2.1 Flink的三种图结构(该部分主要参考[追源索骥：透过源码看懂Flink核心框架的执行流程之2.理解flink的图结构](https://www.cnblogs.com/bethunebtj/p/9168274.html#2%E7%90%86%E8%A7%A3flink%E7%9A%84%E5%9B%BE%E7%BB%93%E6%9E%84))

- StreamGraph
   - StreamGraph是对用户逻辑的映射
   - 代表程序的拓扑结构，是从用户代码直接生成的图
   - client执行env.execute()时生成

- JobGraph
   - JobGraph在StreamGraph基础上进行了一些优化，比如把一部分操作串成chain以提高效率
   - client生成
   - ExecutionGraph已经可以用于调度任务。我们可以看到，flink根据该图生成了一一对应的Task，每个task对应一个ExecutionGraph的一个Execution。Task用InputGate、InputChannel和ResultPartition对应了上面图中的IntermediateResult和ExecutionEdge

- ExecutionGraph
   - ExecutionGraph是为了调度存在的，加入了并行处理的概念
   - JobManager处生成，入口代码是ExecutionGraphBuilder.buildGraph（...）

2.2.2 Flink任务执行架构图

- 参考官网[Job Managers、Task Managers、客户端（Clients）](https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/concepts/runtime.html)

2.2.3 Flink任务的调度和执行

- Flink 集群启动后，首先会启动一个 JobManger 和多个的 TaskManager。用户的代码会由JobClient 提交给 JobManager，JobManager 再把来自不同用户的任务发给 不同的TaskManager 去执行，每个TaskManager管理着多个task，task是执行计算的最小结构， TaskManager 将心跳和统计信息汇报给 JobManager。TaskManager 之间以流的形式进行数据的传输。上述除了task外的三者均为独立的 JVM 进程。
- 要注意的是，TaskManager和job并非一一对应的关系。flink调度的最小单元是task而非TaskManager，也就是说，来自不同job的不同task可能运行于同一个TaskManager的不同线程上


--2.3 备注：该部分主要参考以下内容

- [Cris 带你快速入门 Flink之二 Flink基本架构](https://juejin.im/post/5c4f16dbe51d454f342fb7e7#heading-4)
- [追源索骥：透过源码看懂Flink核心框架的执行流程之3. 任务的调度与执行](https://www.cnblogs.com/bethunebtj/p/9168274.html#3-%E4%BB%BB%E5%8A%A1%E7%9A%84%E8%B0%83%E5%BA%A6%E4%B8%8E%E6%89%A7%E8%A1%8C)
- [Flink 原理与实现：理解 Flink 中的计算资源](http://wuchong.me/blog/2016/05/09/flink-internals-understanding-execution-resources/)


**3 统一的批处理和流处理系统**

--3.1 流处理系统与批处理系统的不同

- 最大不同在于节点间的数据传输方式

- 流处理系统，节点间数据传输模型：当一条数据被处理完成后，序列化到缓存中，然后立刻通过网络传输到下一个节点，由下一个节点继续处理

- 批处理系统，其节点间数据传输模型是：当一条数据被处理完成后，序列化到缓存中，并不会立刻通过网络传输到下一个节点，当缓存写满，就持久化到本地硬盘上，当所有数据都被处理完成后，才开始将处理后的数据通过网络传输到下一个节点

- 流处理中，数据被一个节点处理完成后立即序列化化到缓存中传到下一个节点进行处理；而批处理在一个节点处理完后也会序列化到缓存中但会等到缓存写满并写入硬盘，所有数据处理完后才传到下一个节点

- 流处理对应低延迟的要求，批处理系统对应高吞吐量的要求

--3.2 Flink的执行引擎采用了一种十分灵活的方式，同时支持了上述两种数据传输模型

- 固定大小的缓存块 + 缓存块超时值

- Flink以固定的缓存块为单位进行网络数据传输，用户可以通过缓存块超时值指定缓存块的传输时机

- 缓存块的超时值为0，则Flink的数据传输方式类似上文所提到流处理系统的标准模型，此时系统可以获得最低的处理延迟

- 缓存块的超时值为无限大，则Flink的数据传输方式类似上文所提到批处理系统标准模型，此时系统可以获得最高的吞吐量

- 缓存块的超时值也可以设置为0到无限大之间的任意值。缓存块的超时阀值越小，则Flink流处理执行引擎的数据处理延迟越低，但吞吐量也会降低，反之亦然。通过调整缓存块的超时阀值，用户可根据需求灵活地权衡系统延迟和吞吐量

--3.3 备注：该部分主要参考[Flink 核心技术浅析（整理版）
](https://www.cnblogs.com/swordfall/p/10612404.html)

**4 Flink流处理的容错机制**

--4.1 Flink 提供的容错机制

- 分布式系统的容错：对于一个分布式系统来说，单个进程或是节点崩溃导致整个Job失败是经常发生的事情，在异常发生时不会丢失用户数据并能自动恢复才是分布式系统必须支持的特性之一

- 批处理系统的容错：由于文件可以重复访问，当个某个任务失败后，重启该任务即可

- Flink通过Checkpoint机制实现容错，该思想借鉴了Chandy和Lamport在1985年发表的一篇关于分布式快照的论文

--4.2 分布式快照Chandy Lamport算法逻辑(该部分主要参考[Flink 核心技术浅析（整理版）
之2.2 Flink流处理的容错机制](https://www.cnblogs.com/swordfall/p/10612404.html))

- 按照用户自定义的分布式快照间隔时间，Flink会定时在所有数据源中插入一种特殊的快照标记消息，这些快照标记消息和其他消息一样在DAG中流动，但是不会被用户定义的业务逻辑所处理，每一个快照标记消息都将其所在的数据流分成两部分：本次快照数据和下次快照数据

- 快照标记消息沿着DAG流经各个操作符，当操作符处理到快照标记消息时，会对自己的状态进行快照，并存储起来。当一个操作符有多个输入的时候，Flink会将先抵达的快照标记消息及其之后的消息缓存起来，当所有的输入中对应该快照的快照标记消息全部抵达后，操作符对自己的状态快照并存储，之后处理所有快照标记消息之后的已缓存消息。操作符对自己的状态快照并存储可以是异步与增量的操作，并不需要阻塞消息的处理

- 当所有的Data Sink（终点操作符）都收到快照标记信息并对自己的状态快照和存储后，整个分布式快照就完成了，同时通知数据源释放该快照标记消息之前的所有消息。若之后发生节点崩溃等异常情况时，只需要恢复之前存储的分布式快照状态，并从数据源重发该快照以后的消息就可以了

- 分布式快照的时间间隔越短，错误恢复的时间越少，与吞吐量负相关

--4.3 Flink Checkpoint机制具体实现流程(该部分主要参考：[追源索骥：透过源码看懂Flink核心框架的执行流程之5.1.4 Flink的分布式快照机制](https://www.cnblogs.com/bethunebtj/p/9168274.html#5-%E4%B8%BA%E6%89%A7%E8%A1%8C%E4%BF%9D%E9%A9%BE%E6%8A%A4%E8%88%AAfault-tolerant%E4%B8%8E%E4%BF%9D%E8%AF%81exactly-once%E8%AF%AD%E4%B9%89))

- fault tolerant就是从持久化存储中读取上次记录的这些元信息，并且恢复到程序中

- Flink引入了一个概念，叫做Barrier。Barrier是一种标记，它被source产生并且插入到流数据中，被发送到下游节点。当下游节点处理到该barrier标志时，这就意味着在该barrier插入到流数据时，已经进入系统的数据在当前节点已经被处理完毕

- 每当一个barrier流过一个算子节点时，就说明了在该算子上，可以触发一次检查点，用以保存当前节点的状态和已经处理过的数据，这就是一份快照

- 与此同时，该算子会向下游发送该barrier。因为数据在算子之间是按顺序发送的，所以当下游节点收到该barrier时，也就意味着同样的一批数据在下游节点上也处理完毕，可以进行一次checkpoint，保存基于该节点的一份快照，快照完成后，会通知JobMananger自己完成了这个快照

- 如果有不止一个下游节点，就向每个下游发送barrier

- 如果有不止一个上游节点，那么就要等到所有上游节点的同一批次的barrier到达之后，才能触发checkpoint。因为每个节点运算速度不同，所以有的上游节点可能已经在发下个barrier周期的数据了，有的上游节点还没发送本次的barrier，这时候，当前算子就要缓存一下提前到来的数据，等比较慢的上游节点发送barrier之后，才能处理下一批数据

- 当整个程序的最后一个算子sink都收到了这个barrier，也就意味着这个barrier和上个barrier之间所夹杂的这批元素已经全部落袋为安。这时，最后一个算子通知JobManager整个流程已经完成，而JobManager随后发出通知，要求所有算子删除本次快照内容，以完成清理

--4.4 流处理引擎提供的三种数据处理语义(该部分主要参考：[谈谈流计算中的『Exactly Once』特性](https://zhuanlan.zhihu.com/p/69958793))

- 流处理引擎提供的三种数据处理语义
   - 最多一次（At-most-once）：本质上是一『尽力而为』的方法。保证数据或事件最多由应用程序中的所有算子处理一次。 这意味着如果数据在被流应用程序完全处理之前发生丢失，则不会进行其他重试或者重新发送
   - 至少一次（At-least-once）：应用程序中的所有算子都保证数据或事件至少被处理一次。这通常意味着如果事件在流应用程序完全处理之前丢失，则将从源头重放或重新传输事件。然而，由于事件是可以被重传的，因此一个事件有时会被处理多次，这就是所谓的至少一次
   - 精确一次（Exactly-once）：即使是在各种故障的情况下，流应用程序中的所有算子都保证事件只会被『精确一次』的处理

- 对Exactly-once的理解
   - 事件的处理可以发生多次，但是该处理的效果只在持久后端状态存储中反映一次，流处理引擎管理的算子状态的不同更新只反映一次

--4.5 Flink通过两阶段提交协议提供端到端的Exactly-Once保证(该部分主要参考：[深入理解Flink ---- End-to-End Exactly-Once语义](https://www.cnblogs.com/tuowang/p/9025266.html))

- Exactly-Once语义是针对Flink系统内部而言的，结合Kafka如何构建端到端的Exactly-Once处理

- 两阶段提交协议
   - 第一阶段Phase 1，Pre-commit：Flink的JobManager向source注入checkpoint barrier以开启这次snapshot，barrier从source流向sink，每个进行snapshot的算子成功snapshot后,都会向JobManager发送ACK，当sink完成snapshot后, 向JobManager发送ACK的同时向kafka进行pre-commit
   - 第二阶段Phase 2，Commit：当JobManager接收到所有算子的ACK后,就会通知所有的算子这次checkpoint已经完成，Sink接收到这个通知后, 就向kafka进行commit,正式把数据写入到kafka

- 不同阶段错误的恢复
   - 在pre-commit前fail over, 系统恢复到最近的checkponit
   - 在pre-commit后,commit前fail over,系统恢复到刚完成pre-commit时的状态

- Flink的two phase commit实现：抽象类TwoPhaseCommitSinkFunction
   - beginTransaction()：开启事务.创建一个临时文件.后续把原要写入到外部系统的数据写入到这个临时文件
   - preCommit()：flush并close这个文件,之后便不再往其中写数据.同时开启一个新的事务供下个checkponit使用
   - commit()：把pre-committed的临时文件移动到指定目录
   - abort()：删除掉pre-committed的临时文件


**5 Flink流处理的时间窗口**

--5.1 Flink的时间模型

- EventTime是数据被生产出来的时间，可以是比如传感器发出信号的时间等（此时数据还没有被传输给flink）

- IngestionTime是数据进入flink的时间，也就是从Source进入flink流的时间（此时数据刚刚被传给flink）

- ProcessingTime是针对当前算子的系统时间，是指该数据已经进入某个operator时，operator所在系统的当前时间

--5.2 乱序问题

- 乱序问题一般是和EventTime关联的，对于一个流式处理系统的process time来说，是不存在乱序问题的。所以下面介绍的watermark/allowedLateness也只是在EventTime作为主时间才生效

- Flink通过watermark+window+trigger解决乱序问题
   - window解决的是where，也就是将无界数据划分成有界数据(window提供了allowedLateness方法，使得更大限度的允许乱序)
   - trigger用来设计窗口数据触发条件
   - watermark用来标记窗口的完整性

--5.3 watermark+window+trigger解决乱序问题

- watermark(水位线)
   - watermark是流式系统中主要用于解决流式系统中数据乱序问题的机制，方法是用于标记当前处理到什么水位的数据了，这意味着再早于这个水位的数据过来会被直接丢弃。这使得引擎可以自动跟踪数据中的当前事件时间，并尝试相应地清除旧状态
   - watermark和数据本身一样作为正常的消息在流中流动

- trigger
   - trigger指明在哪些条件下触发window计算，基于处理数据时的时间以及事件的特定属性。一般trigger的实现是当watermark处于某种时间条件下或者窗口数据达到一定条件，窗口的数据开始计算
   - 每次trigger对新增的数据相关的window进行重新计算，输出有complete, append,update三种输出模式
      - Complete mode(全量模式)：Result Table 全量输出，也就是重新计算过的window结果都输出。意味着这种模式下，每次读了新增的input数据，output的时候会把内存中resulttable中所有window的结果都输出一遍
      - Append mode (default)(增量模式)：只有 Result Table 中新增的行才会被输出，所谓新增是指自上一次 trigger 的时候。因为只是输出新增的行，所以如果老数据有改动就不适合使用这种模式。 更新的window并不输出，否则外存里的key就重了
      - Update mode(更新模式)：只要更新的 Row 都会被输出，相当于 Append mode 的加强版。而且是对外存中的相同key进行update，而不是append，需要外存是能kv操作的！只会输出新增和更新过的window的结果

- window的allowedLateness
   - allowedLateness就是针对event time而言，对于watermark超过end-of-window之后，还允许有一段时间（也是以event time来衡量）来等待之前的数据到达，以便再次处理这些数据
   - watermark是全局性的参数，用于管理消息的乱序，watermark超过window的endtime之后，就会触发窗口计算。一般情况下，触发窗口计算之后，窗口就销毁掉了，后面再来的数据也不会再计算，对于trigger是默认的EventTimeTrigger的情况下，allowedLateness会再次触发窗口的计算，而之前触发的数据，会buffer起来，直到watermark超过end-of-window + allowedLateness的时间，窗口的数据及元数据信息才会被删除
   - 比如window的endtime是5000，allowedLateness=0，那么如果watermark 5000到来之后，这个window就应该被清除。但是如果allowedLateness = 1000，则需要等water 6000(endtime + allowedLateness)到来之后，这个window才会被清掉

--5.4 watermark的生成和传递

- watermark的生成
   - Flink通过水位线分配器（TimestampsAndPeriodicWatermarksOperator和TimestampsAndPunctuatedWatermarksOperator这两个算子）向事件流中注入水位线。元素在streaming dataflow引擎中流动到WindowOperator时，会被分为两拨，分别是普通事件和水位线

- watermark的传递
   - 算子接受到的水印是来自其数据上游所有watermark的最小值
   - watermark以广播的形式在算子之间传播，当一个算子收到watermark时都要
      - 更新算子时间
      - 遍历计时器队列触发回调
      - 将watermark发送到下游

--5.5 备注：该部分主要参考[[源码分析] 从源码入手看 Flink Watermark 之传播过程](https://www.cnblogs.com/rossiXYZ/p/12345969.html)