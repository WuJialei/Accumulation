# 关于Kafka的学习总结

**1 Kafka的概念及特点**

--1.1 概念

- Kafka是一个分布式MQ(消息队列)系统

- Kafka是一个分布式、分区的、多副本的、多订阅者的日志系统，可以用于web/nginx日志，搜索日志，监控日志，访问日志等等

--1.2 特点

- 高吞吐量、低延迟

- 持久性、可靠性

- 可扩展性：Kafka集群支持热扩展

- 容错性：允许集群中节点失败（若副本数量为n,则允许n-1个节点失败）

- 高并发：支持数千个客户端同时读写

**2 Kafka的架构**

--2.1 Kafka架构图及组成部分

- 架构图参见[3、 Kafka 的设计架构你知道吗？](https://www.iteblog.com/archives/2605.html#32Kafka)

- 组成部分
   - Producer ：消息生产者，就是向 Kafka broker 发消息的客户端
   - Broker ：一台 Kafka 服务器就是一个 broker。一个Kafka集群由多个 broker 组成。一个 broker 可以容纳多个 topic
   - Topic ：主题，可以理解为一个队列，一个 Topic 又分为一个或多个分区(Partition)
   - Partition：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列。partition中的每条消息都会被分配一个有序的id（offset）。kafka只保证按一个partition中的顺序将消息发给consumer(单个partition里的消息保证有序性)，不保证一个topic的整体（多个partition间）的顺序；一个partition可以有多个副本replica，kafka中,replication策略是基于partition,而不是topic;kafka将每个partition数据复制到多个server上,任何一个partition有一个leader和多个follower(可以没有);备份的个数可以通过broker配置文件来设定.leader处理所有的read-write请求,follower需要和leader保持同步.Follower就像一个"consumer",消费消息并保存在本地日志中;leader负责跟踪所有的follower状态,如果follower"落后"太多或者失效,leader将会把它从replicas同步列表中删除.当所有的follower都将一条消息保存成功,此消息才被认为是"committed",那么此时consumer才能消费它,这种同步策略,就要求follower和leader之间必须具有良好的网络环境.即使只有一个replicas实例存活,仍然可以保证消息的正常发送和接收,只要zookeeper集群存活即可.(备注:不同于其他分布式存储,比如hbase需要"多数派"存活才行)
   - Consumer ：消息消费者，向kafka broker取消息的客户端
   - Consumer Group （CG）：这是kafka用来实现一个topic消息的广播（发给所有的consumer）和单播（发给任意一个consumer）的手段。一个topic可以有多个CG。topic的消息会复制（不是真的复制，是概念上的）到所有的CG，但每个CG只会把消息发给该CG中的一个consumer。如果需要实现广播，只要每个consumer有一个独立的CG就可以了。要实现单播只要所有的consumer在同一个CG。用CG还可以将consumer进行自由的分组而不需要多次发送消息到不同的topic。对于Topic中的一条特定的消息,只会被订阅此Topic的每个group中的一个consumer消费,此消息不会发送给一个group的多个consumer;那么一个group中所有的consumer将会交错的消费整个Topic.在kafka中,一个partition中的消息只会被group中的一个consumer消费(同一时刻);每个group中consumer消息消费互相独立;我们可以认为一个group是一个"订阅"者,一个Topic中的每个partions,只会被一个"订阅者"中的一个consumer消费,不过一个consumer可以同时消费多个partitions中的消息.kafka只能保证一个partition中的消息被某个consumer消费时是顺序的.事实上,从Topic角度来说,当有多个partitions时,消息仍不是全局有序的。通常情况下,一个group中会包含多个consumer,这样不仅可以提高topic中消息的并发消费能力,而且还能提高"故障容错"性,如果group中的某个consumer失效,那么其消费的partitions将会有其他consumer自动接管.kafka的设计原理决定,对于一个topic,同一个group中不能有多于partitions个数的consumer同时消费,否则将意味着某些consumer将无法得到消息
   - Offset：kafka 的存储文件都是按照 offset.kafka 来命名，用 offset 做名字的好处是方便查找。例如你想找位于 2049 的位置，只要找到 2048.kafka 的文件即可。当然 the first offset 就是 00000000000.kafka
   
- Kafka中相关概念解释
   - ISR：In-Sync Replicas，(分区partition)副本同步队列
   - AR：Assigned Replicas，所有副本
   - OSR：Out-of-Sync Replicas，ISR是由(分区partition)中的leader维护，follower从leader同步数据有一些延迟，超过相应的阈值会把 follower 剔除出 ISR, 存入OSR（Out-of-Sync Replicas ）列表，新加入的follower也会先存放在OSR中。AR=ISR+OSR
   - LEO：是 LogEndOffset 的简称，代表当前日志文件中下一条，表示log中最后的message的offst位置.
   - HW：水位或水印（watermark）一词，也可称为高水位(high watermark)，通常被用在流式处理领域（比如Apache Flink、Apache Spark等），以表征元素或事件在基于时间层面上的进度。在Kafka中，水位的概念反而与时间无关，而是与位置信息相关。严格来说，它表示的就是位置信息，即位移（offset）。取 partition 对应的 ISR中 最小的 LEO 作为 HW，示Partition各个replicas数据间同步且一致的offset位置，即表示all replicas已经commit位置，每个Broker缓存中维护此信息,并不断更新，consumer 最多只能消费到 HW 所在的位置上一条信息

--2.2 Kafka消息的Pull/Push模式

- producer将消息推送到broker，consumer从broker拉取消息

- 好处
   - 由consumer决定消息的消费速率，当broker推送的速率远大于consumer消费的速率时，consumer恐怕就要崩溃了
   - consumer可以自主决定是否批量的从broker拉取数据。Push模式必须在不知道下游consumer消费能力和消费策略的情况下决定是立即推送每条消息还是缓存之后批量推送。如果为了避免consumer崩溃而采用较低的推送速率，将可能导致一次只推送较少的消息而造成浪费。Pull模式下，consumer就可以根据自己的消费能力去决定这些策略

--2.3 Kafka创建Topic时如何将分区partition放置到哪个Broker的策略、分区partition放置到对应Broker目录下的策略、同一个 Consumer Group 里面的 Consumer 消费分区partition的策略

- 将分区partition放置到哪个Broker的策略
   - 每个主题topic有多个分区partition，每个分区partition会存在多个副本replica，因此需要讨论的是这些副本replica是怎么样放置在 Kafka 集群的 Broker 中的
   - 一些博客介绍的方法(不准确)
      - Kafka尽量将所有的Partition均匀分配到整个集群上。Kafka分配Replica的算法如下：将所有存活的N个Brokers和待分配的Partition排序；将第i个Partition分配到第(i mod n)个Broker上，这个Partition的第一个Replica存在于这个分配的Broker上，并且会作为partition的优先副本；将第i个Partition的第j个Replica分配到第((i + j) mod n)个Broker上
      - 缺点：所有主题的第一个分区都是存放在第一个Broker上，这样会造成第一个Broker上的分区总数多于其他的Broker，这样就失去了负载均衡的目的；如果主题的分区数多于Broker的个数，多于的分区都是倾向于将分区发放置在前几个Broker上，同样导致负载不均衡
   - 真正采用的方法
      - 副本因子不能大于 Broker 的个数
      - 第一个分区（编号为0）的第一个副本放置位置是随机从 brokerList 选择的
      - 其他分区的第一个副本放置位置相对于第0个分区依次往后移。也就是如果我们有5个 Broker，5个分区，假设第一个分区放在第四个 Broker 上，那么第二个分区将会放在第五个 Broker 上；第三个分区将会放在第一个 Broker 上；第四个分区将会放在第二个 Broker 上，依次类推
      - 剩余的副本相对于第一个副本放置位置其实是由 nextReplicaShift 决定的，而这个数也是随机产生的
      - 举例
      ~~~
        assignReplicasToBrokers(Seq(0, 1, 2, 3), 5, 3) // (broker列表、partition分区个数，replica副本个数)
        (0,List(3, 1, 2))
        (1,List(0, 2, 3))
        (2,List(1, 3, 0))
        (3,List(2, 0, 1))
        (4,List(3, 2, 0))
      ~~~
   
- 分区partition放置到对应Broker目录下的策略
   - 在启动 Kafka 集群之前，broker需要配置好 log.dirs 参数，其值是 Kafka 数据的存放目录，这个参数可以配置多个目录，目录之间使用逗号分隔，通常这些目录是分布在不同的磁盘上用于提高读写性能。当然也可以配置 log.dir 参数，含义一样。只需要设置其中一个即可
   - 如果 log.dirs 参数只配置了一个目录，那么分配到各个 Broker 上的分区肯定只能在这个目录下创建文件夹用于存放数据
   - 如果 log.dirs 参数配置了多个目录，Kafka 会在含有分区目录最少的文件夹中创建新的分区目录，分区目录名为 Topic名+分区ID。注意，是分区文件夹总数最少的目录，而不是磁盘使用量最少的目录！也就是说，如果给 log.dirs 参数新增了一个新的磁盘，新的分区目录肯定是先在这个新的磁盘上创建直到这个新的磁盘目录拥有的分区目录不是最少为止
   - 缺点：
      - 分区数最少的目录未必是数据量最少的目录，如果分区数最少的目录恰恰是数据量最多的目录这样会导致磁盘使用不均衡
      - 这种实现也没有考虑到磁盘的读写负载

- 同一个 Consumer Group 里面的 Consumer 消费分区partition的策略
   - Kafka 存在 Consumer Group 的概念，也就是 group.id 一样的 Consumer，组内的所有消费者协调在一起来消费订阅主题(subscribed topics)的所有分区(partition)。当然，每个分区只能由同一个消费组内的一个consumer来消费
   - 同一个 Consumer Group 里面的 Consumer 消费Partition的内部分区分配策略（Partition Assignment Strategy）
      - Kafka 内部存在两种默认的分区分配策略：Range 和 RoundRobin
      - 不能自定义分区分配策略，只能通过partition.assignment.strategy参数选择 range 或 roundrobin。partition.assignment.strategy参数默认的值是range
      - Range策略
         - 是对每个主题而言的，首先对同一个主题里面的分区按照序号进行排序，并对消费者按照字母顺序进行排序。在我们的例子里面，排完序的分区将会是0, 1, 2, 3, 4, 5, 6, 7, 8, 9；消费者线程排完序将会是C1-0, C2-0, C2-1。然后将partitions的个数除于消费者线程的总数来决定每个消费者线程消费几个分区。如果除不尽，那么前面几个消费者线程将会多消费一个分区。在我们的例子里面，我们有10个分区，3个消费者线程， 10 / 3 = 3，而且除不尽，那么消费者线程 C1-0 将会多消费一个分区，所以最后分区分配的结果看起来是这样的：
         ~~~
            C1-0 将消费 0, 1, 2, 3 分区
            C2-0 将消费 4, 5, 6 分区
            C2-1 将消费 7, 8, 9 分区
         ~~~
         - 假如我们有2个主题(T1和T2)，分别有10个分区，那么最后分区分配的结果看起来是这样的：
         ~~~
            C1-0 将消费 T1主题的 0, 1, 2, 3 分区以及 T2主题的 0, 1, 2, 3分区
            C2-0 将消费 T1主题的 4, 5, 6 分区以及 T2主题的 4, 5, 6分区
            C2-1 将消费 T1主题的 7, 8, 9 分区以及 T2主题的 7, 8, 9分区
            可以看出，C1-0 消费者线程比其他消费者线程多消费了2个分区，这就是Range strategy的一个很明显的弊端 
         ~~~           
      - RoundRobin策略
         - 有两个前提条件必须满足：同一个Consumer Group里面的所有消费者的num.streams必须相等；每个消费者订阅的主题必须相同
         - 将所有主题的分区组成 TopicAndPartition 列表，然后对 TopicAndPartition 列表按照 hashCode 进行排序
         - 举例
         ~~~
         按照 hashCode 排序完的topic-partitions组依次为T1-5, T1-3, T1-0, T1-8, T1-2, T1-1, T1-4, T1-7, T1-6, T1-9，我们的消费者线程排序为C1-0, C1-1, C2-0, C2-1，最后分区分配的结果为：

            C1-0 将消费 T1-5, T1-2, T1-6 分区；
            C1-1 将消费 T1-3, T1-1, T1-9 分区；
            C2-0 将消费 T1-0, T1-4 分区；
            C2-1 将消费 T1-8, T1-7 分区；
         ~~~

- Kafka Producer是动态感知Topic分区Partition数的变化
   - 使用Kafka Producer往Kafka的Broker发送消息的时候，Kafka会根据消息的key计算出这条消息应该发送到哪个分区。默认的分区计算类是HashPartitioner，其实现如下：
   ~~~
   class HashPartitioner(props: VerifiableProperties = null) extends Partitioner {
        def partition(data: Any, numPartitions: Int): Int = {
            (data.hashCode % numPartitions)
        }
    }
   ~~~
   - numPartitions就是Tpoic的分区总数。partition函数会在kafka的getPartition函数中被调用，计算消息的分区ID
   - 如果在Kafka Producer往Kafka的Broker发送消息的时候用户通过命令修改了改主题的分区数，Kafka Producer能动态感知吗？答案是可以的。那是立刻就感知吗？不是，是过一定的时间(topic.metadata.refresh.interval.ms参数决定)才知道分区数改变的
   - 如果topicMetadataRefreshInterval>=0并且当前时间减去上一次元数据更新的时间间隙大于topicMetadataRefreshInterval，则会再一次更新Tpoic的元数据，而topicMetadataRefreshInterval的值就是通过topic.metadata.refresh.interval.ms配置的
   - 在启动Kafka Producer往Kafka的Broker发送消息的时候，用户修改了该Topic的分区数，Producer可以在最多topic.metadata.refresh.interval.ms的时间之后感知到，此感知同时适用于async和sync模式，并且可以将数据发送到新添加的分区中

- Kafka 的partition分区数可以增加但不能减少
   - 可以使用 bin/kafka-topics.sh 命令对 Kafka 增加 Kafka 的分区数据，但是 Kafka 不支持减少分区数。Kafka 分区数据不支持减少是由很多原因的，比如减少的分区其数据放到哪里去？是删除，还是保留？删除的话，那么这些没消费的消息不就丢了。如果保留这些消息如何放到其他分区里面？追加到其他分区后面的话那么就破坏了 Kafka 单个分区的有序性。如果要保证删除分区数据插入到其他分区保证有序性，那么实现起来逻辑就会非常复杂

--2.4  Kafka分区Partition副本的Follower如何与分区Partition副本的Leader同步数据

- Kafka的复制机制既不是完全的同步复制，也不是单纯的异步复制。完全同步复制要求All Alive Follower都复制完，这条消息才会被认为commit，这种复制方式极大的影响了吞吐率。而异步复制方式下，Follower异步的从Leader复制数据，数据只要被Leader写入log就被认为已经commit，这种情况下如果Follower都复制完都落后于Leader，而如果Leader突然宕机，则会丢失数据。而Kafka的这种使用ISR的方式则很好的均衡了确保数据不丢失以及吞吐率。Follower可以批量的从Leader复制数据，而且Leader充分利用磁盘顺序读以及send file(zero copy)机制，这样极大的提高复制性能，内部批量写磁盘，大幅减少了Follower与Leader的消息量差

- 优缺点
   - 优点
      - 性能高，吞吐量大
      - 降低了系统和磁盘开销，Leader充分利用磁盘顺序读以及send file(zero copy)机制
      - 降低Leader与Follower之间网络开销和交互次数
   - 缺点
      - 有可能会占用大量网络带宽(例如本来集群很大而且数据量很多，后来新增Broker节点需要迁移数据)，甚至堵塞网络，需要有流控机制，否则会影响线上服务
      - 因为Follower是批量拉取Leader消息，如果设置为保证所有replicas commit，才返回Ack给生产者会存在抖动现象，Follow拉取Leader修改HW，当HW与当次生产者请求logEndOffset的offst一致时，客户端等待时间会拉长

- 原理：Kafka中partition replication之间同步数据，从partition的leader复制数据到follower只需要一个线程(ReplicaFetcherThread)，实际上复制是follower(一个follower相当于consumer)主动从leader批量拉取消息的，这极大提高了吞吐量

- kafka判定一个follower存活与否的条件有2个
   - follower需要和zookeeper保持良好的链接 
   - follower必须能够及时的跟进leader,不能落后太多
   - 同时满足上述2个条件,那么leader就认为此follower是"活跃的".如果一个follower失效(server失效)或者落后太多,leader将会把它从同步列表中移除[备注:如果此replicas落后太多,它将会继续从leader中fetch数据,直到足够up-to-date,然后再次加入到同步列表中;kafka不会更换replicas宿主!因为"同步列表ISR"中replicas需要足够快,这样才能保证producer发布消息时接受到ACK的延迟较小

- leader失效后的选举
   - leader失效时,需在followers中选取出新的leader,可能此时follower落后于leader,因此需要选择一个"up-to-date"的follower
   - 整个集群中,只要有一个replicas存活,那么此partition都可以继续接受读写操作
   - 对于kafka而言,每个partition中所有的replicas信息都可以在zookeeper中获得,那么选举leader将是一件非常简单的事情.选择follower时需要兼顾一个问题,就是新leader server上所已经承载的partition leader的个数,如果一个server上有过多的partition leader,意味着此server将承受着更多的IO压力.在选举新leader,需要考虑到"负载均衡",partition leader较少的broker将会更有可能成为新的leader

- 分区的副本与leader不同步的原因与检测
   - 原因
      - 慢副本：在一定周期时间内follower不能追赶上leader。最常见的原因之一是I / O瓶颈导致follower追加复制消息速度慢于从leader拉取速度
      - 卡住副本：在一定周期时间内follower停止从leader拉取请求。follower replica卡住了是由于GC暂停或follower失效或死亡
      - 新启动副本：当用户给主题增加副本因子时，新的follower不在同步副本列表中，直到他们完全赶上了leader日志
   - 检测
      - 一个partition的follower落后于leader足够多时，被认为不在同步副本列表或处于滞后状态。在Kafka-0.8.2.x中,副本滞后判断依据是副本落后于leader最大消息数量(replica.lag.max.messages)或replicas响应partition leader的最长等待时间(replica.lag.time.max.ms)。前者是用来检测缓慢的副本,而后者是用来检测失效或死亡的副本
      - 真正重要的事情是检测卡或慢副本,这段时间follower replica是“out-of-sync”落后于leader。在服务端现在只有一个参数需要配置replica.lag.time.max.ms。这个参数解释replicas响应partition leader的最长等待时间。检测卡住或失败副本的探测——如果一个replica失败导致发送拉取请求时间间隔超过replica.lag.time.max.ms。Kafka会认为此replica已经死亡会从同步副本列表从移除。检测慢副本机制发生了变化——如果一个replica开始落后leader超过replica.lag.time.max.ms。Kafka会认为太缓慢并且会从同步副本列表中移除。除非replica请求leader时间间隔大于replica.lag.time.max.ms，因此即使leader使流量激增和大批量写消息。Kafka也不会从同步副本列表从移除该副本

--2.5 Partition分区的存储方式(结构)

- 参考[Kafka文件存储机制那些事](https://tech.meituan.com/2015/01/13/kafka-fs-design-theory.html)

- 在Kafka文件存储中，同一个topic下有多个不同partition，每个partition为一个目录，partiton命名规则为topic名称+有序序号，第一个partiton序号从0开始，序号最大值为partitions数量减1

- 每个partion(目录)相当于一个巨型文件被平均分配到多个大小相等segment(段)数据文件中
   - 每个段segment file消息数量不一定相等，这种特性方便old segment file快速被删除
   - 每个partiton只需要支持顺序读写就行了，segment文件生命周期由服务端配置参数决定
   - 好处：能快速删除无用文件，有效提高磁盘利用率

- segment file由2大部分组成，分别为index file和data file，此2个文件一一对应，成对出现，后缀”.index”和“.log”分别表示为segment索引文件、数据文件
   - segment文件命名规则：partion全局的第一个segment从0开始，后续每个segment文件名为上一个segment文件最后一条消息的offset值。数值最大为64位long大小，19位数字字符长度，没有数字用0填充
   - 索引文件存储大量元数据，数据文件存储大量消息
   - 索引文件中元数据指向对应数据文件中message的物理偏移地址
   - segment index file采取稀疏索引存储方式，它减少索引文件大小，通过mmap可以直接内存操作，稀疏索引为数据文件的每个对应message设置一个元数据指针,它比稠密索引节省了更多的存储空间，但查找起来需要消耗更多的时间

- partition中如何通过offset查找message
   - 第一步，定位到具体segment index文件；根据offset通过二分查找文件列表，就可以快速定位到具体文件
   - 第二步，通过第一步定位到的segment index file查找到message的物理地址，从而直接在segment log file定位到message，当offset=368776时，依次定位到00000000000000368769.index的元数据物理位置和00000000000000368769.log的物理偏移地址，然后再通过00000000000000368769.log顺序查找直到offset=368776为止

- Kafka高效文件存储设计特点
   - Kafka把topic中一个parition大文件分成多个小文件段，通过多个小文件段，就容易定期清除或删除已经消费完文件，减少磁盘占用
   - 通过索引信息可以快速定位message和确定response的最大大小
   - 通过index元数据全部映射到memory，可以避免segment file的IO磁盘操作
   - 通过索引文件稀疏存储，可以大幅降低index文件元数据占用空间大小

--2.6 Kafka中的负载均衡

- Producer均衡算法
   - kafka集群中的任何一个broker,都可以向producer提供metadata信息,这些metadata中包含"集群中存活的servers列表"/"partitions leader列表"等信息(请参看zookeeper中的节点信息).当producer获取到metadata信息之后, producer将会和Topic下所有partition leader保持socket连接;消息由producer直接通过socket发送到broker,中间不会经过任何"路由层".事实上,消息被路由到哪个partition上,有producer客户端决定.比如可以采用"random""key-hash""轮询"等,如果一个topic中有多个partitions,那么在producer端实现"消息均衡分发"是必要的.在producer端的配置文件中,开发者可以指定partition路由的方式

- Consumer均衡算法
   - 当一个group中,有consumer加入或者离开时,会触发partitions均衡.均衡的最终目的,是提升topic的并发消费能力
   - 假如topic1,具有如下partitions: P0,P1,P2,P3；2) 加入group中,有如下consumer: C0,C1；3) 首先根据partition索引号对partitions排序: P0,P1,P2,P3；4) 根据consumer.id排序: C0,C1；5) 计算倍数: M = [P0,P1,P2,P3].size / [C0,C1].size,本例值M=2(向上取整)；6) 然后依次分配partitions: C0 = [P0,P1],C1=[P2,P3],即Ci = [P(i * M),P((i + 1) * M -1)]

--2.7 本部分参考

- [apache Kafka概要介绍](https://blog.csdn.net/lizhitao/article/details/23743821)
- [32 道常见的 Kafka 面试题你都会吗？附答案](https://www.iteblog.com/archives/2605.html#32Kafka)
- [Kafka创建Topic时如何将分区放置到不同的Broker中](https://www.iteblog.com/archives/2219.html)
- [Kafka新建的分区会在哪个目录下创建](https://www.iteblog.com/archives/2231.html)
- [Kafka Producer是如何动态感知Topic分区数变化](https://www.iteblog.com/archives/1618.html)
- [Apache Kafka中Follower如何与Leader同步数据](https://blog.csdn.net/lizhitao/article/details/45066437)
- [Kafka文件存储机制那些事](https://tech.meituan.com/2015/01/13/kafka-fs-design-theory.html)
- [Kafka副本同步机制理解](https://blog.csdn.net/lizhitao/article/details/51718185)

**3 Kafka高吞吐的实现**

--3.1 高吞吐设计
- Kafka是分布式消息系统，需要处理海量的消息，Kafka的设计是把所有的消息都写入速度低容量大的硬盘，以此来换取更强的存储能力，但实际上，使用硬盘并没有带来过多的性能损失。kafka主要使用了以下几个方式实现了超高的吞吐率

- 顺序读写
   - Kafka的消息是不断追加到文件中的，这个特性使kafka可以充分利用磁盘的顺序读写性能。顺序读写不需要硬盘磁头的寻道时间，只需很少的扇区旋转时间，所以速度远快于随机读写

- 零拷贝
   - "零拷贝(zero-copy)"系统调用机制，就是跳过“用户缓冲区”的拷贝，建立一个磁盘空间和内存的直接映射，数据不再复制到“用户态缓冲区”。系统上下文切换减少为2次，可以提升一倍的性能
   - 零拷贝知识具体参见：[深入剖析Linux IO原理和几种零拷贝机制的实现](https://juejin.im/post/5d84bd1f6fb9a06b2d780df7#heading-18)

- 文件分段
   - kafka的队列topic被分为了多个区partition，每个partition又分为多个段segment，所以一个队列中的消息实际上是保存在N多个片段文件中
   - 通过分段的方式，每次文件操作都是对一个小文件的操作，非常轻便，同时也增加了并行处理能力

- 批量发送
   - Kafka允许进行批量发送消息，先将消息缓存在内存中，然后一次请求批量发送出去。比如可以指定缓存的消息达到某个量的时候就发出去，或者缓存了固定的时间后就发送出去。如100条消息就发送，或者每5秒发送一次。这种策略将大大减少服务端的I/O次数

- 数据压缩
   - Kafka还支持对消息集合进行压缩，Producer可以通过GZIP或Snappy格式对消息集合进行压缩。压缩的好处就是减少传输的数据量，减轻对网络传输的压力。Producer压缩之后，在Consumer需进行解压，虽然增加了CPU的工作，但在对大数据处理上，瓶颈在网络上而不是CPU，所以这个成本很值得

--3.2 参考

- [Kafka是如何实现高吞吐率的](https://cloud.tencent.com/developer/article/1083674)
- [深入剖析Linux IO原理和几种零拷贝机制的实现](https://juejin.im/post/5d84bd1f6fb9a06b2d780df7#heading-18)

**4 Kafka数据可靠性和一致性实现原理**

--4.1 数据可靠性的实现原理

- producer 级别：acks=all（或者 request.required.acks=-1），同时发生模式为同步 producer.type=sync
   - 为了让用户设置数据可靠性， Kafka 在 Producer 里面提供了消息确认机制。可以在定义 Producer 时通过 acks 参数指定（在 0.8.2.X 版本之前是通过 request.required.acks 参数设置）
      - acks = 0：意味着如果生产者能够通过网络把消息发送出去，那么就认为消息已成功写入 Kafka 。在这种情况下还是有可能发生错误，比如发送的对象无能被序列化或者网卡发生故障，但如果是分区离线或整个集群长时间不可用，那就不会收到任何错误。在 acks=0 模式下的运行速度是非常快的（这就是为什么很多基准测试都是基于这个模式），你可以得到惊人的吞吐量和带宽利用率，不过如果选择了这种模式， 一定会丢失一些消息
      - acks = 1：意味若 Leader 在收到消息并把它写入到分区数据文件（不一定同步到磁盘上）时会返回确认或错误响应。在这个模式下，如果发生正常的 Leader 选举，生产者会在选举时收到一个 LeaderNotAvailableException 异常，如果生产者能恰当地处理这个错误，它会重试发送悄息，最终消息会安全到达新的 Leader 那里。不过在这个模式下仍然有可能丢失数据，比如消息已经成功写入 Leader，但在消息被复制到 follower 副本之前 Leader发生崩溃
      - acks = all（这个和 request.required.acks = -1 含义一样）：意味着 Leader 在返回确认或错误响应之前，会等待所有同步副本都收到悄息。如果和 min.insync.replicas 参数结合起来，就可以决定在返回确认前至少有多少个副本能够收到悄息，生产者会一直重试直到消息被成功提交。不过这也是最慢的做法，因为生产者在继续发送其他消息之前需要等待所有副本都收到当前的消息
   - Producer 发送消息还可以选择同步（默认，通过 producer.type=sync 配置） 或者异步（producer.type=async）模式。如果设置成异步，虽然会极大的提高消息发送的性能，但是这样会增加丢失数据的风险。如果需要确保消息的可靠性，必须将 producer.type 设置为 sync

- topic 级别：设置 replication.factor>=3，并且 min.insync.replicas>=2
   - Kafka 的分区多副本架构是 Kafka 可靠性保证的核心，把消息写入多个副本可以使 Kafka 在发生崩溃时仍能保证消息的持久性

- broker 级别：关闭不完全的 Leader 选举，即 unclean.leader.election.enable=false
   - 每个分区的 leader 会维护一个 ISR 列表，ISR 列表里面就是 follower 副本的 Borker 编号，只有跟得上 Leader 的 follower 副本才能加入到 ISR 里面，这个是通过 replica.lag.time.max.ms 参数配置的。只有 ISR 里的成员才有被选为 leader 的可能
   - 当 Leader 挂掉了，而且 unclean.leader.election.enable=false 的情况下，Kafka 会从 ISR 列表中选择第一个 follower 作为新的 Leader，因为这个分区拥有最新的已经 committed 的消息。通过这个可以保证已经 committed 的消息的数据可靠性

--4.2 数据一致性的实现原理

- 引入了 High Watermark 机制，High WaterMark 取决于 ISR 列表里面偏移量最小的分区，很类似于木桶原理

- 引入了 High Water Mark 机制，会导致 Broker 间的消息复制因为某些原因变慢，那么消息到达消费者的时间也会随之变长（因为我们会先等待消息复制完毕）。延迟时间可以通过参数 replica.lag.time.max.ms 参数配置，它指定了副本在复制消息时可被允许的最大延迟时间

--4.3 参考

- [Kafka 是如何保证数据可靠性和一致性](https://www.iteblog.com/archives/2560.html)

**5 Kafka的幂等性与Exactly Once**

--5.1 Kafka的delivery guarantee

- 之前的旧版本中，Kafka只能支持两种语义：At most once和At least once。At most once保证消息不会朝服，但是可能会丢失。在实践中，很有有业务会选择这种方式。At least once保证消息不会丢失，但是可能会重复，业务在处理消息需要进行去重

- Kafka在0.11.0.0版本支持增加了对幂等的支持。幂等是针对生产者角度的特性。幂等可以保证上生产者发送的消息，不会丢失，而且不会重复

--5.2 幂等性的定义和实现关键

- 定义：一次和多次请求某一个资源对于资源本身应该具有同样的结果（网络超时等问题除外）。也就是说，其任意多次执行对资源本身所产生的影响均与一次执行的影响相同

- 实现关键：实现幂等的关键点就是服务端可以区分请求是否重复，过滤掉重复的请求。要区分请求是否重复的有两点
   - 唯一标识：要想区分请求是否重复，请求中就得有唯一标识。例如支付请求中，订单号就是唯一标识
   - 记录下已处理过的请求标识：光有唯一标识还不够，还需要记录下那些请求是已经处理过的，这样当收到新的请求时，用新请求中的标识和处理记录进行比较，如果处理记录中有相同的标识，说明是重复交易，拒绝掉

--5.3 Kafka实现幂等性原理

- 为了实现Producer的幂等性，Kafka引入了Producer ID（即PID）和Sequence Numbe
   - PID，每个新的Producer在初始化的时候会被分配一个唯一的PID，这个PID对用户是不可见的
   - Sequence Numbler，（对于每个PID，该Producer发送数据的每个<Topic, Partition>都对应一个从0开始单调递增的Sequence Number

- Kafka可能存在多个生产者，会同时产生消息，但对Kafka来说，只需要保证每个生产者内部的消息幂等就可以了，所有引入了PID来标识不同的生产者

- 对于Kafka来说，要解决的是生产者发送消息的幂等问题。也即需要区分每条消息是否重复。
Kafka通过为每条消息增加一个Sequence Numbler，通过Sequence Numbler来区分每条消息。每条消息对应一个分区，不同的分区产生的消息不可能重复。所有Sequence Numbler对应每个分区

- Broker端在缓存中保存了这seq number，对于接收的每条消息，如果其序号比Broker缓存中序号大于1则接受它，否则将其丢弃。这样就可以实现了消息重复提交了。但是，只能保证单个Producer对于同一个<Topic, Partition>的Exactly Once语义。不能保证同一个Producer一个topic不同的partion幂等

--5.4 本部分参考

- [Kafka幂等性介绍与源码实现](https://www.jianshu.com/p/b1599f46229b)
- [Kafka设计解析（八）- Exactly Once语义与事务机制原理](http://www.jasongj.com/kafka/transaction/)

**6 参考**

- [apache kafka技术分享系列(目录索引)](https://blog.csdn.net/lizhitao/article/details/39499283)
- [32 道常见的 Kafka 面试题你都会吗？附答案](https://www.iteblog.com/archives/2605.html#32Kafka)